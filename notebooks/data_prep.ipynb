{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook is for preparing the PAMAP2 dataset for training PyTorch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 4.83 ms\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading all data into a single dataframe\n",
    "Since there is a lot of data, this might take a while (about 1 min 20 secs on a cluster server)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0   1   2     3        4        5        6        7        8        9   \\\n",
      "0  5.70   0 NaN  34.0  2.22755  9.65418  2.38862  2.31968  9.60752  2.58278   \n",
      "1  5.71   0 NaN  34.0  2.37550  9.57647  2.31412  2.45657  9.62177  2.71852   \n",
      "2  5.72   0 NaN  34.0  2.94208  9.53415  2.32275  2.78876  9.64961  2.76342   \n",
      "3  5.73   0 NaN  34.0  3.47541  9.75837  2.40696  3.33080  9.66073  2.68734   \n",
      "4  5.74   0 NaN  34.0  3.54617  9.83232  2.25382  3.67642  9.71848  2.50565   \n",
      "\n",
      "   ...        44        45        46       47       48       49        50  \\\n",
      "0  ... -0.017907  0.009340  0.050097 -32.7091  31.4772  44.2318  0.255373   \n",
      "1  ... -0.070091  0.002312  0.053833 -33.0782  30.9814  44.5148  0.251163   \n",
      "2  ... -0.084468  0.034249  0.030462 -32.5619  30.6982  44.5169  0.250643   \n",
      "3  ... -0.030789  0.058615  0.055252 -32.8212  30.9690  44.6575  0.250917   \n",
      "4  ... -0.020063  0.020903  0.059653 -33.1869  30.0856  44.5154  0.249631   \n",
      "\n",
      "         51        52        53  \n",
      "0  0.783075  0.084602  0.560732  \n",
      "1  0.785583  0.076319  0.560314  \n",
      "2  0.786419  0.072038  0.559941  \n",
      "3  0.786967  0.071013  0.559179  \n",
      "4  0.786602  0.066861  0.560777  \n",
      "\n",
      "[5 rows x 54 columns]\n",
      "time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "all_data_list = list()\n",
    "for file in os.listdir('../data/original/'):\n",
    "    # check whether file to be loaded is csv \n",
    "    # and also ensure no other files are attempted to be parsed.\n",
    "    if file[-4 : ] == '.csv':\n",
    "        df = pd.read_csv(os.path.join('../data/original/', file), names = range(54))\n",
    "        all_data_list.append(df)\n",
    "        \n",
    "# Combine all the dataframes\n",
    "all_data_df = pd.concat(all_data_list)\n",
    "\n",
    "print(all_data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unnecessary columns \n",
    "There is a lot of data (other IMU sensors except the one on the chest) which needs to be dropped. So, the columns which are not required are dropped in the next block. Refer to the `readme.pdf` in the PAMAP2_Dataset (available online) to verify which columns were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0   1        21       22       23\n",
      "0  5.70   0  1.45391  5.87561 -7.88825\n",
      "1  5.71   0  1.56970  5.98847 -7.84780\n",
      "2  5.72   0  1.52482  6.02469 -8.08106\n",
      "3  5.73   0  1.43944  6.02286 -8.39237\n",
      "4  5.74   0  1.24887  6.02345 -8.39582\n",
      "     0   1        21       22       23\n",
      "0  5.70   0  1.45391  5.87561 -7.88825\n",
      "1  5.71   0  1.56970  5.98847 -7.84780\n",
      "2  5.72   0  1.52482  6.02469 -8.08106\n",
      "3  5.73   0  1.43944  6.02286 -8.39237\n",
      "4  5.74   0  1.24887  6.02345 -8.39582\n",
      "time: 40.6 s\n"
     ]
    }
   ],
   "source": [
    "# drop data of IMU sensor mounted on hand, temperature reading of chest IMU sensor and heart rate sensor reading\n",
    "all_data_df.drop(range(2, 21), axis = 1, inplace = True)\n",
    "# drop data of IMU sensor mounted on ankle and unnecessary data of chest IMU sensor\n",
    "all_data_df.drop(range(24, 54), axis = 1, inplace = True)\n",
    "print(all_data_df.head())\n",
    "\n",
    "# Due to combination of multiple dataframes, the indices remained the same from the original dataframe\n",
    "# even in the full all_data_df dataframe. So, the indices needed to be reset. Upon resetting the indices\n",
    "# get converted into a column called 'index', so it needs to removed as it is unnecessary. \n",
    "all_data_df.reset_index(inplace = True)\n",
    "all_data_df.drop('index', axis = 1, inplace = True)\n",
    "print(all_data_df.head())\n",
    "\n",
    "# Indices are again reset so we can have a column of the correct indices which are required to \n",
    "# eliminate the unnecessary rows (which contained CI and proximity data only and no accelerometer data)\n",
    "all_data_df.reset_index(inplace = True)\n",
    "all_data_df.to_csv('../data/cleaned_new.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 612 Âµs\n"
     ]
    }
   ],
   "source": [
    "# print(all_data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unnecessary rows \n",
    "The creators of the dataset mention the existence of unavailable data due to hardware failure (represented by NaN in the data). These entries need to be removed. Further, there is a 'transient' class (label = 0) that needs to be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of indices where timestamp is not NaN but accelerometer data is NaN \n",
    "# i.e. indices which don't have any accelerometer data\n",
    "drop_indices = list()\n",
    "for t, x, ind, label in zip(all_data_df.iloc[:, 1], all_data_df.iloc[:, 3], all_data_df.iloc[:, 0], all_data_df.iloc[:, 2]):\n",
    "    if pd.isna(x) or label == 0 or pd.isna(t):\n",
    "        drop_indices.append(ind)\n",
    "        \n",
    "# Then, drop those indices and drop the 'index' column created earlier since it is not needed now.\n",
    "all_data_df.drop(all_data_df.index[drop_indices], inplace = True)\n",
    "all_data_df.drop('index', axis = 1, inplace = True)\n",
    "# saving to file for further use\n",
    "all_data_df.to_csv('../data/cleaned_new2.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unnecessary columns and normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28.3 s\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/cleaned_new2.csv', names = ['time', 'High_level', 'linear_acc.x', 'linear_acc.y', 'linear_acc.z'], header = 0)\n",
    "\n",
    "# We won't drop the timestamps as they are still required to separate the examples later\n",
    "# df.drop(['time'], axis = 1, inplace = True)\n",
    "\n",
    "# normalizing acceleration data using factor of 16384 mentioned in datasheet of MPU6050\n",
    "# to get the acceleration in multiples of 'g' (9.8 m/s^2)\n",
    "# df['linear_acc.x'] = df['linear_acc.x'] / 16384.0\n",
    "# df['linear_acc.y'] = df['linear_acc.y'] / 16384.0\n",
    "# df['linear_acc.z'] = df['linear_acc.z'] / 16384.0\n",
    "\n",
    "# using sklearn min_max_scaler\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x = df['linear_acc.x'].values.reshape(-1, 1)\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df['linear_acc.x'] = x_scaled\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x = df['linear_acc.y'].values.reshape(-1, 1)\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df['linear_acc.y'] = x_scaled\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x = df['linear_acc.z'].values.reshape(-1, 1)\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df['linear_acc.z'] = x_scaled\n",
    "\n",
    "# print(df.head())\n",
    "df.to_csv('../data/cleaned_new3.csv', index = None, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
