{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook is for preparing the PAMAP2 dataset for training PyTorch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading all data into a single dataframe\n",
    "Since there is a lot of data, this might take a while (about 1 min 20 secs on a cluster server)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0   1   2     3        4        5        6        7        8        9   \\\n",
      "0  5.70   0 NaN  34.0  2.22755  9.65418  2.38862  2.31968  9.60752  2.58278   \n",
      "1  5.71   0 NaN  34.0  2.37550  9.57647  2.31412  2.45657  9.62177  2.71852   \n",
      "2  5.72   0 NaN  34.0  2.94208  9.53415  2.32275  2.78876  9.64961  2.76342   \n",
      "3  5.73   0 NaN  34.0  3.47541  9.75837  2.40696  3.33080  9.66073  2.68734   \n",
      "4  5.74   0 NaN  34.0  3.54617  9.83232  2.25382  3.67642  9.71848  2.50565   \n",
      "\n",
      "   ...        44        45        46       47       48       49        50  \\\n",
      "0  ... -0.017907  0.009340  0.050097 -32.7091  31.4772  44.2318  0.255373   \n",
      "1  ... -0.070091  0.002312  0.053833 -33.0782  30.9814  44.5148  0.251163   \n",
      "2  ... -0.084468  0.034249  0.030462 -32.5619  30.6982  44.5169  0.250643   \n",
      "3  ... -0.030789  0.058615  0.055252 -32.8212  30.9690  44.6575  0.250917   \n",
      "4  ... -0.020063  0.020903  0.059653 -33.1869  30.0856  44.5154  0.249631   \n",
      "\n",
      "         51        52        53  \n",
      "0  0.783075  0.084602  0.560732  \n",
      "1  0.785583  0.076319  0.560314  \n",
      "2  0.786419  0.072038  0.559941  \n",
      "3  0.786967  0.071013  0.559179  \n",
      "4  0.786602  0.066861  0.560777  \n",
      "\n",
      "[5 rows x 54 columns]\n",
      "time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "all_data_list = list()\n",
    "for file in os.listdir('../data/original/'):\n",
    "    # check whether file to be loaded is csv \n",
    "    # and also ensure no other files are attempted to be parsed.\n",
    "    if file[-4 : ] == '.csv':\n",
    "        df = pd.read_csv(os.path.join('../data/original/', file), names = range(54))\n",
    "        all_data_list.append(df)\n",
    "        \n",
    "# Combine all the dataframes\n",
    "all_data_df = pd.concat(all_data_list)\n",
    "\n",
    "print(all_data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unnecessary columns \n",
    "There is a lot of data (other IMU sensors except the one on the chest) which needs to be dropped. So, the columns which are not required are dropped in the next block. Refer to the `readme.pdf` in the PAMAP2_Dataset (available online) to verify which columns were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0   1        21       22       23\n",
      "0  5.70   0  1.45391  5.87561 -7.88825\n",
      "1  5.71   0  1.56970  5.98847 -7.84780\n",
      "2  5.72   0  1.52482  6.02469 -8.08106\n",
      "3  5.73   0  1.43944  6.02286 -8.39237\n",
      "4  5.74   0  1.24887  6.02345 -8.39582\n",
      "     0   1        21       22       23\n",
      "0  5.70   0  1.45391  5.87561 -7.88825\n",
      "1  5.71   0  1.56970  5.98847 -7.84780\n",
      "2  5.72   0  1.52482  6.02469 -8.08106\n",
      "3  5.73   0  1.43944  6.02286 -8.39237\n",
      "4  5.74   0  1.24887  6.02345 -8.39582\n",
      "time: 40.6 s\n"
     ]
    }
   ],
   "source": [
    "# drop data of IMU sensor mounted on hand, temperature reading of chest IMU sensor and heart rate sensor reading\n",
    "all_data_df.drop(range(2, 21), axis = 1, inplace = True)\n",
    "# drop data of IMU sensor mounted on ankle and unnecessary data of chest IMU sensor\n",
    "all_data_df.drop(range(24, 54), axis = 1, inplace = True)\n",
    "print(all_data_df.head())\n",
    "\n",
    "# Due to combination of multiple dataframes, the indices remained the same from the original dataframe\n",
    "# even in the full all_data_df dataframe. So, the indices needed to be reset. Upon resetting the indices\n",
    "# get converted into a column called 'index', so it needs to removed as it is unnecessary. \n",
    "all_data_df.reset_index(inplace = True)\n",
    "all_data_df.drop('index', axis = 1, inplace = True)\n",
    "print(all_data_df.head())\n",
    "\n",
    "# Indices are again reset so we can have a column of the correct indices which are required to \n",
    "# eliminate the unnecessary rows (which contained CI and proximity data only and no accelerometer data)\n",
    "all_data_df.reset_index(inplace = True)\n",
    "all_data_df.to_csv('../data/cleaned_new.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 612 Âµs\n"
     ]
    }
   ],
   "source": [
    "# print(all_data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unnecessary rows \n",
    "The creators of the dataset mention the existence of unavailable data due to hardware failure (represented by NaN in the data). These entries need to be removed. Further, there is a 'transient' class (label = 0) that needs to be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of indices where timestamp is not NaN but accelerometer data is NaN \n",
    "# i.e. indices which don't have any accelerometer data\n",
    "drop_indices = list()\n",
    "for t, x, ind, label in zip(all_data_df.iloc[:, 1], all_data_df.iloc[:, 3], all_data_df.iloc[:, 0], all_data_df.iloc[:, 2]):\n",
    "    if pd.isna(x) or label == 0 or pd.isna(t):\n",
    "        drop_indices.append(ind)\n",
    "        \n",
    "# Then, drop those indices and drop the 'index' column created earlier since it is not needed now.\n",
    "all_data_df.drop(all_data_df.index[drop_indices], inplace = True)\n",
    "all_data_df.drop('index', axis = 1, inplace = True)\n",
    "# saving to file for further use\n",
    "all_data_df.to_csv('../data/cleaned_new2.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unnecessary columns and normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28.3 s\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/cleaned_new2.csv', names = ['time', 'High_level', 'linear_acc.x', 'linear_acc.y', 'linear_acc.z'], header = 0)\n",
    "\n",
    "# We won't drop the timestamps as they are still required to separate the examples later\n",
    "# df.drop(['time'], axis = 1, inplace = True)\n",
    "\n",
    "# normalizing acceleration data using factor of 16384 mentioned in datasheet of MPU6050\n",
    "# to get the acceleration in multiples of 'g' (9.8 m/s^2)\n",
    "# df['linear_acc.x'] = df['linear_acc.x'] / 16384.0\n",
    "# df['linear_acc.y'] = df['linear_acc.y'] / 16384.0\n",
    "# df['linear_acc.z'] = df['linear_acc.z'] / 16384.0\n",
    "\n",
    "# using sklearn min_max_scaler\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x = df['linear_acc.x'].values.reshape(-1, 1)\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df['linear_acc.x'] = x_scaled\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x = df['linear_acc.y'].values.reshape(-1, 1)\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df['linear_acc.y'] = x_scaled\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x = df['linear_acc.z'].values.reshape(-1, 1)\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df['linear_acc.z'] = x_scaled\n",
    "\n",
    "# print(df.head())\n",
    "df.to_csv('../data/cleaned_new3.csv', index = None, header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important : Read this before executing next cell !\n",
    "\n",
    "**Delete the files which are already generated before running the scripts to generate them again. This is because the code appends to the file so if you re-run the scripts without deleting the earlier data, it will append to the earlier data.**\n",
    "\n",
    "This saved data `cleaned_new3.csv` will be passed through `preprocess.py` (in `../code/`) to get `new_padded_data.csv` for further processing below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      x_acc     y_acc     z_acc  label__1  label__2  label__3  label__4  \\\n",
      "0  0.600076  0.283347  0.649533         1         0         0         0   \n",
      "1  0.602326  0.283343  0.648932         1         0         0         0   \n",
      "2  0.599470  0.283345  0.648904         1         0         0         0   \n",
      "3  0.602812  0.283335  0.647378         1         0         0         0   \n",
      "4  0.602726  0.283329  0.645819         1         0         0         0   \n",
      "\n",
      "   label__5  label__6  label__7  label__12  label__13  label__16  label__17  \\\n",
      "0         0         0         0          0          0          0          0   \n",
      "1         0         0         0          0          0          0          0   \n",
      "2         0         0         0          0          0          0          0   \n",
      "3         0         0         0          0          0          0          0   \n",
      "4         0         0         0          0          0          0          0   \n",
      "\n",
      "   label__24  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n",
      "3          0  \n",
      "4          0  \n",
      "time: 58.3 s\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/new_padded_data.csv', names = ['x_acc', 'y_acc', 'z_acc', 'label'])\n",
    "# print(df.head())\n",
    "labels = pd.DataFrame(pd.get_dummies(df['label'], prefix = 'label_'))\n",
    "# print(labels.head())\n",
    "\n",
    "# labels.to_csv('../data/labels.csv', index = None)\n",
    "# assert(len(labels) == len(df))\n",
    "\n",
    "df.drop(['label'], axis = 1, inplace = True)\n",
    "dataframe = pd.concat([df, labels], axis = 1).reset_index(drop = True)\n",
    "print(dataframe.head())\n",
    "dataframe.to_csv('../data/new_final_data.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffling the dataset\n",
    "The examples need to be shuffled before splitting the dataset into different sets. However, the examples should be shuffled and not the samples. So, we reshape the dataframe into 3D and shuffle along one axis such that the samples in each example are maintained but the examples themselves get shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1935100, 15)\n",
      "time: 4.72 s\n"
     ]
    }
   ],
   "source": [
    "final = pd.read_csv('../data/new_final_data.csv', header = 0)\n",
    "# shuffling the dataset before splitting into train, val and test sets\n",
    "three_d = final.values.reshape(-1, 100, final.shape[1])\n",
    "# print(three_d.shape)\n",
    "np.random.seed()\n",
    "np.random.shuffle(three_d)\n",
    "two_d = three_d.reshape(-1, final.shape[1])\n",
    "print(two_d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining samples for 80 : 10 : 10 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548000\n",
      "193500\n",
      "time: 2.24 ms\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 100\n",
    "train_samples = int((0.8 * two_d.shape[0] // reqd_len) * reqd_len) \n",
    "# 1550250 for 80 %\n",
    "print(train_samples)\n",
    "test_val_samples = int((0.1 * two_d.shape[0] // reqd_len) * reqd_len)\n",
    "# 193750 for 10 %\n",
    "print(test_val_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting into training, validation and testing sets, and saving into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15480\n",
      "1935\n",
      "1936\n",
      "time: 45 s\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(two_d[ : 1548000])\n",
    "val_df = pd.DataFrame(two_d[1548000 : 1548000 + 193500])\n",
    "test_df = pd.DataFrame(two_d[1548000 + 193500 : ])\n",
    "print(len(train_df) // reqd_len)\n",
    "print(len(val_df) // reqd_len)\n",
    "print(len(test_df) // reqd_len)\n",
    "train_df.to_csv('../data/train.csv', index = False, header = False)\n",
    "val_df.to_csv('../data/val.csv', index = False, header = False)\n",
    "test_df.to_csv('../data/test.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the distribution of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_0  count\n",
      "label       \n",
      "1       1919\n",
      "2       1847\n",
      "3       1896\n",
      "4       2373\n",
      "5        977\n",
      "6       1641\n",
      "7       1875\n",
      "12      1166\n",
      "13      1040\n",
      "16      1749\n",
      "17      2380\n",
      "24       488\n",
      "time: 2.75 s\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/new_padded_data.csv', names = ['x_acc', 'y_acc', 'z_acc', 'label'])\n",
    "\n",
    "train_outcome = pd.crosstab(index = df[\"label\"], columns = \"count\") // 100\n",
    "print(train_outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving data and labels separately for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "final = pd.read_csv('../data/new_final_data.csv', header = 0)\n",
    "all_data = final.iloc[:, : 3]\n",
    "all_labels = final.iloc[:, 3 : ]\n",
    "all_data.to_csv('../data/data_only_keras.csv', index = False, header = False)\n",
    "all_labels.to_csv('../data/labels_only_keras.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
