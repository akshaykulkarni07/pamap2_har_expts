{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "from torch.autograd import Variable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Class (loads data from csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1550250, 15)\n",
      "(193750, 15)\n",
      "(193850, 15)\n"
     ]
    }
   ],
   "source": [
    "reqd_len = 50\n",
    "channels = 3\n",
    "classes = 12\n",
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, mode = 'test', transform = None):\n",
    "        if mode == 'train' :\n",
    "            self.df = pd.read_csv('../data/train.csv', header = None)\n",
    "        elif mode == 'test' :\n",
    "            self.df = pd.read_csv('../data/test.csv', header = None)\n",
    "        elif mode == 'val' :\n",
    "            self.df = pd.read_csv('../data/val.csv', header = None)\n",
    "        self.transform = transform\n",
    "        print(self.df.shape)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y = self.df.iloc[idx : idx + reqd_len, 3 : ].values\n",
    "        ind = np.argmax(np.sum(y, axis = 0))\n",
    "        label = np.zeros_like(self.df.iloc[0, 3 : ].values)\n",
    "        label = label.astype('float')\n",
    "        label[ind] = 1\n",
    "        x = self.df.iloc[idx : idx + reqd_len, : channels].values\n",
    "        x = x.astype('float')\n",
    "        x = x.reshape(reqd_len, channels)\n",
    "        assert(x.shape == (reqd_len, channels))\n",
    "        assert(label.shape == (classes, ))\n",
    "        return x, label\n",
    "        \n",
    "trainset = IMUDataset(mode = 'train')\n",
    "valset = IMUDataset(mode = 'val')\n",
    "testset = IMUDataset(mode = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader definitions (provides data in iterable form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16\n",
    "batch_size = 16\n",
    "train_indices = [(i * reqd_len) for i in range(len(trainset) // reqd_len)]\n",
    "val_indices = [(i * reqd_len) for i in range(len(valset) // reqd_len)]\n",
    "test_indices = [(i * reqd_len) for i in range(len(testset) // reqd_len)]\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size = train_batch_size, sampler = SubsetRandomSampler(train_indices), drop_last = True)\n",
    "valloader = DataLoader(valset, batch_size = batch_size, sampler = SubsetRandomSampler(val_indices), drop_last = True)\n",
    "testloader = DataLoader(testset, batch_size = batch_size, sampler = SubsetRandomSampler(test_indices), drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def output_size(n, f, p = 0, s = 1):\n",
    "    ''' Returns output size for given input size (n), filter size (f), padding (p) and stride (s)\n",
    "    for a convolutional layer\n",
    "    '''\n",
    "    return (((n + 2 * p - f) / s) + 1)\n",
    "\n",
    "output_size(50, 5)\n",
    "output_size(46, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on GPU\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # defining layers\n",
    "        self.conv1 = nn.Conv1d(3, 5, 3)\n",
    "        self.conv2 = nn.Conv1d(5, 10, 3)\n",
    "        self.fc1 = nn.Linear(46 * 10, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.pamap = nn.Linear(64, 12)\n",
    "        self.robogame = nn.Linear(64, 4)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.conv1.weight, gain = nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self.conv2.weight, gain = nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self.fc1.weight, gain = nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self.fc2.weight, gain = nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self.pamap.weight, gain = nn.init.calculate_gain('sigmoid'))\n",
    "        nn.init.xavier_uniform_(self.robogame.weight, gain = nn.init.calculate_gain('sigmoid'))\n",
    "        \n",
    "    # use flag = True during fine-tuning \n",
    "    def forward(self, signal, flag = False):\n",
    "        signal = torch.transpose(signal, 1, 2)\n",
    "        out = F.relu(self.conv1(signal))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = torch.transpose(out, 1, 2)\n",
    "        out = out.reshape(-1, 46 * 10)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        if flag : \n",
    "            out = F.log_softmax(self.robogame(out), dim = 1)\n",
    "        else :\n",
    "            out = F.log_softmax(self.pamap(out), dim = 1)\n",
    "        return out\n",
    "\n",
    "Net = ConvNet()\n",
    "if torch.cuda.is_available():\n",
    "    print('Model on GPU')\n",
    "    Net = Net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(Net.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  step =  0  of total steps  1937  loss =  2.453301191329956\n",
      "epoch =  0  step =  200  of total steps  1937  loss =  2.200176954269409\n",
      "epoch =  0  step =  400  of total steps  1937  loss =  2.323225498199463\n",
      "epoch =  0  step =  600  of total steps  1937  loss =  2.1629562377929688\n",
      "epoch =  0  step =  800  of total steps  1937  loss =  2.0657637119293213\n",
      "epoch =  0  step =  1000  of total steps  1937  loss =  2.148059368133545\n",
      "epoch =  0  step =  1200  of total steps  1937  loss =  1.8734049797058105\n",
      "epoch =  0  step =  1400  of total steps  1937  loss =  1.949501633644104\n",
      "epoch =  0  step =  1600  of total steps  1937  loss =  2.1992881298065186\n",
      "epoch =  0  step =  1800  of total steps  1937  loss =  1.568833589553833\n",
      "epoch :  0  /  100  | TL :  2.054805278716533  | VL :  1.7599546909332275\n",
      "saving model\n",
      "epoch =  1  step =  0  of total steps  1937  loss =  2.008435010910034\n",
      "epoch =  1  step =  200  of total steps  1937  loss =  2.0438590049743652\n",
      "epoch =  1  step =  400  of total steps  1937  loss =  1.5398294925689697\n",
      "epoch =  1  step =  600  of total steps  1937  loss =  1.2426893711090088\n",
      "epoch =  1  step =  800  of total steps  1937  loss =  2.131204605102539\n",
      "epoch =  1  step =  1000  of total steps  1937  loss =  2.1130125522613525\n",
      "epoch =  1  step =  1200  of total steps  1937  loss =  1.6469666957855225\n",
      "epoch =  1  step =  1400  of total steps  1937  loss =  1.4702723026275635\n",
      "epoch =  1  step =  1600  of total steps  1937  loss =  1.433138132095337\n",
      "epoch =  1  step =  1800  of total steps  1937  loss =  1.9936432838439941\n",
      "epoch :  1  /  100  | TL :  1.579350453587494  | VL :  1.458680272102356\n",
      "saving model\n",
      "epoch =  2  step =  0  of total steps  1937  loss =  1.4227001667022705\n",
      "epoch =  2  step =  200  of total steps  1937  loss =  1.5896161794662476\n",
      "epoch =  2  step =  400  of total steps  1937  loss =  1.1361713409423828\n",
      "epoch =  2  step =  600  of total steps  1937  loss =  1.1032477617263794\n",
      "epoch =  2  step =  800  of total steps  1937  loss =  1.5437884330749512\n",
      "epoch =  2  step =  1000  of total steps  1937  loss =  1.2762235403060913\n",
      "epoch =  2  step =  1200  of total steps  1937  loss =  1.4526963233947754\n",
      "epoch =  2  step =  1400  of total steps  1937  loss =  1.1606353521347046\n",
      "epoch =  2  step =  1600  of total steps  1937  loss =  1.2310295104980469\n",
      "epoch =  2  step =  1800  of total steps  1937  loss =  1.128798246383667\n",
      "epoch :  2  /  100  | TL :  1.357293504904273  | VL :  1.439931869506836\n",
      "saving model\n",
      "epoch =  3  step =  0  of total steps  1937  loss =  1.4187300205230713\n",
      "epoch =  3  step =  200  of total steps  1937  loss =  1.6502101421356201\n",
      "epoch =  3  step =  400  of total steps  1937  loss =  0.8912624716758728\n",
      "epoch =  3  step =  600  of total steps  1937  loss =  1.291195273399353\n",
      "epoch =  3  step =  800  of total steps  1937  loss =  1.2421786785125732\n",
      "epoch =  3  step =  1000  of total steps  1937  loss =  1.0828016996383667\n",
      "epoch =  3  step =  1200  of total steps  1937  loss =  1.318610429763794\n",
      "epoch =  3  step =  1400  of total steps  1937  loss =  1.0782757997512817\n",
      "epoch =  3  step =  1600  of total steps  1937  loss =  1.1419330835342407\n",
      "epoch =  3  step =  1800  of total steps  1937  loss =  1.2199903726577759\n",
      "epoch :  3  /  100  | TL :  1.2716679419508654  | VL :  1.3188836574554443\n",
      "saving model\n",
      "epoch =  4  step =  0  of total steps  1937  loss =  1.7903257608413696\n",
      "epoch =  4  step =  200  of total steps  1937  loss =  1.7132084369659424\n",
      "epoch =  4  step =  400  of total steps  1937  loss =  1.0657260417938232\n",
      "epoch =  4  step =  600  of total steps  1937  loss =  1.148781657218933\n",
      "epoch =  4  step =  800  of total steps  1937  loss =  1.4718934297561646\n",
      "epoch =  4  step =  1000  of total steps  1937  loss =  0.9898661375045776\n",
      "epoch =  4  step =  1200  of total steps  1937  loss =  1.2569187879562378\n",
      "epoch =  4  step =  1400  of total steps  1937  loss =  1.2498546838760376\n",
      "epoch =  4  step =  1600  of total steps  1937  loss =  1.0565792322158813\n",
      "epoch =  4  step =  1800  of total steps  1937  loss =  1.3234187364578247\n",
      "epoch :  4  /  100  | TL :  1.2280146389753472  | VL :  1.1810579299926758\n",
      "saving model\n",
      "epoch =  5  step =  0  of total steps  1937  loss =  1.0190255641937256\n",
      "epoch =  5  step =  200  of total steps  1937  loss =  1.3513444662094116\n",
      "epoch =  5  step =  400  of total steps  1937  loss =  1.1472909450531006\n",
      "epoch =  5  step =  600  of total steps  1937  loss =  1.265040397644043\n",
      "epoch =  5  step =  800  of total steps  1937  loss =  1.1793477535247803\n",
      "epoch =  5  step =  1000  of total steps  1937  loss =  1.653279185295105\n",
      "epoch =  5  step =  1200  of total steps  1937  loss =  1.1837331056594849\n",
      "epoch =  5  step =  1400  of total steps  1937  loss =  1.1976207494735718\n",
      "epoch =  5  step =  1600  of total steps  1937  loss =  0.8378927707672119\n",
      "epoch =  5  step =  1800  of total steps  1937  loss =  1.1656625270843506\n",
      "epoch :  5  /  100  | TL :  1.1918339038024444  | VL :  1.177605390548706\n",
      "saving model\n",
      "epoch =  6  step =  0  of total steps  1937  loss =  1.431045413017273\n",
      "epoch =  6  step =  200  of total steps  1937  loss =  1.0473374128341675\n",
      "epoch =  6  step =  400  of total steps  1937  loss =  0.8021185398101807\n",
      "epoch =  6  step =  600  of total steps  1937  loss =  1.289554238319397\n",
      "epoch =  6  step =  800  of total steps  1937  loss =  1.1349040269851685\n",
      "epoch =  6  step =  1000  of total steps  1937  loss =  1.0630923509597778\n",
      "epoch =  6  step =  1200  of total steps  1937  loss =  0.7204689979553223\n",
      "epoch =  6  step =  1400  of total steps  1937  loss =  1.0370124578475952\n",
      "epoch =  6  step =  1600  of total steps  1937  loss =  1.2471394538879395\n",
      "epoch =  6  step =  1800  of total steps  1937  loss =  1.5414611101150513\n",
      "epoch :  6  /  100  | TL :  1.158752179770477  | VL :  1.1115893125534058\n",
      "saving model\n",
      "epoch =  7  step =  0  of total steps  1937  loss =  1.5029852390289307\n",
      "epoch =  7  step =  200  of total steps  1937  loss =  1.2013322114944458\n",
      "epoch =  7  step =  400  of total steps  1937  loss =  1.6744999885559082\n",
      "epoch =  7  step =  600  of total steps  1937  loss =  0.9570324420928955\n",
      "epoch =  7  step =  800  of total steps  1937  loss =  0.6625463962554932\n",
      "epoch =  7  step =  1000  of total steps  1937  loss =  0.7114032506942749\n",
      "epoch =  7  step =  1200  of total steps  1937  loss =  1.2984663248062134\n",
      "epoch =  7  step =  1400  of total steps  1937  loss =  0.8430729508399963\n",
      "epoch =  7  step =  1600  of total steps  1937  loss =  1.052800178527832\n",
      "epoch =  7  step =  1800  of total steps  1937  loss =  1.2308281660079956\n",
      "epoch :  7  /  100  | TL :  1.1395974470803463  | VL :  1.256568193435669\n",
      "epoch =  8  step =  0  of total steps  1937  loss =  1.1983873844146729\n",
      "epoch =  8  step =  200  of total steps  1937  loss =  1.238879680633545\n",
      "epoch =  8  step =  400  of total steps  1937  loss =  1.157233715057373\n",
      "epoch =  8  step =  600  of total steps  1937  loss =  1.5581860542297363\n",
      "epoch =  8  step =  800  of total steps  1937  loss =  1.3047258853912354\n",
      "epoch =  8  step =  1000  of total steps  1937  loss =  1.1014785766601562\n",
      "epoch =  8  step =  1200  of total steps  1937  loss =  1.6604807376861572\n",
      "epoch =  8  step =  1400  of total steps  1937  loss =  0.8606796264648438\n",
      "epoch =  8  step =  1600  of total steps  1937  loss =  0.7006360292434692\n",
      "epoch =  8  step =  1800  of total steps  1937  loss =  1.9686421155929565\n",
      "epoch :  8  /  100  | TL :  1.1225300417865867  | VL :  1.1493327617645264\n",
      "epoch =  9  step =  0  of total steps  1937  loss =  0.8346092104911804\n",
      "epoch =  9  step =  200  of total steps  1937  loss =  1.2653002738952637\n",
      "epoch =  9  step =  400  of total steps  1937  loss =  1.0876418352127075\n",
      "epoch =  9  step =  600  of total steps  1937  loss =  1.226248025894165\n",
      "epoch =  9  step =  800  of total steps  1937  loss =  1.4873276948928833\n",
      "epoch =  9  step =  1000  of total steps  1937  loss =  1.4786789417266846\n",
      "epoch =  9  step =  1200  of total steps  1937  loss =  1.0433298349380493\n",
      "epoch =  9  step =  1400  of total steps  1937  loss =  0.9426568150520325\n",
      "epoch =  9  step =  1600  of total steps  1937  loss =  1.4177377223968506\n",
      "epoch =  9  step =  1800  of total steps  1937  loss =  1.2757760286331177\n",
      "epoch :  9  /  100  | TL :  1.1052052583131953  | VL :  1.0928834676742554\n",
      "saving model\n",
      "epoch =  10  step =  0  of total steps  1937  loss =  1.2181638479232788\n",
      "epoch =  10  step =  200  of total steps  1937  loss =  0.7294749021530151\n",
      "epoch =  10  step =  400  of total steps  1937  loss =  0.9003568291664124\n",
      "epoch =  10  step =  600  of total steps  1937  loss =  0.8873851895332336\n",
      "epoch =  10  step =  800  of total steps  1937  loss =  1.2573283910751343\n",
      "epoch =  10  step =  1000  of total steps  1937  loss =  1.5354620218276978\n",
      "epoch =  10  step =  1200  of total steps  1937  loss =  0.7023330926895142\n",
      "epoch =  10  step =  1400  of total steps  1937  loss =  1.1327874660491943\n",
      "epoch =  10  step =  1600  of total steps  1937  loss =  0.7588592171669006\n",
      "epoch =  10  step =  1800  of total steps  1937  loss =  0.7126112580299377\n",
      "epoch :  10  /  100  | TL :  1.0988723685985136  | VL :  1.1245074272155762\n",
      "epoch =  11  step =  0  of total steps  1937  loss =  1.1045979261398315\n",
      "epoch =  11  step =  200  of total steps  1937  loss =  0.8311662077903748\n",
      "epoch =  11  step =  400  of total steps  1937  loss =  1.124869465827942\n",
      "epoch =  11  step =  600  of total steps  1937  loss =  0.8657252788543701\n",
      "epoch =  11  step =  800  of total steps  1937  loss =  1.1211477518081665\n",
      "epoch =  11  step =  1000  of total steps  1937  loss =  0.9057821035385132\n",
      "epoch =  11  step =  1200  of total steps  1937  loss =  0.9759411215782166\n",
      "epoch =  11  step =  1400  of total steps  1937  loss =  1.1626548767089844\n",
      "epoch =  11  step =  1600  of total steps  1937  loss =  1.0992436408996582\n",
      "epoch =  11  step =  1800  of total steps  1937  loss =  1.3803476095199585\n",
      "epoch :  11  /  100  | TL :  1.0812417514156791  | VL :  1.0602319240570068\n",
      "saving model\n",
      "epoch =  12  step =  0  of total steps  1937  loss =  0.6103037595748901\n",
      "epoch =  12  step =  200  of total steps  1937  loss =  1.3145114183425903\n",
      "epoch =  12  step =  400  of total steps  1937  loss =  0.9958413243293762\n",
      "epoch =  12  step =  600  of total steps  1937  loss =  0.805684506893158\n",
      "epoch =  12  step =  800  of total steps  1937  loss =  1.5656298398971558\n",
      "epoch =  12  step =  1000  of total steps  1937  loss =  1.1525248289108276\n",
      "epoch =  12  step =  1200  of total steps  1937  loss =  0.7737000584602356\n",
      "epoch =  12  step =  1400  of total steps  1937  loss =  0.9812108874320984\n",
      "epoch =  12  step =  1600  of total steps  1937  loss =  1.0488874912261963\n",
      "epoch =  12  step =  1800  of total steps  1937  loss =  0.4398919343948364\n",
      "epoch :  12  /  100  | TL :  1.0787727090428647  | VL :  1.114801049232483\n",
      "epoch =  13  step =  0  of total steps  1937  loss =  1.1263976097106934\n",
      "epoch =  13  step =  200  of total steps  1937  loss =  1.212375521659851\n",
      "epoch =  13  step =  400  of total steps  1937  loss =  1.2479878664016724\n",
      "epoch =  13  step =  600  of total steps  1937  loss =  1.2551440000534058\n",
      "epoch =  13  step =  800  of total steps  1937  loss =  1.1081428527832031\n",
      "epoch =  13  step =  1000  of total steps  1937  loss =  0.8432702422142029\n",
      "epoch =  13  step =  1200  of total steps  1937  loss =  1.7632275819778442\n",
      "epoch =  13  step =  1400  of total steps  1937  loss =  1.2095400094985962\n",
      "epoch =  13  step =  1600  of total steps  1937  loss =  0.9044914245605469\n",
      "epoch =  13  step =  1800  of total steps  1937  loss =  0.9273263216018677\n",
      "epoch :  13  /  100  | TL :  1.0679681007709463  | VL :  1.0617069005966187\n",
      "epoch =  14  step =  0  of total steps  1937  loss =  1.1494600772857666\n",
      "epoch =  14  step =  200  of total steps  1937  loss =  0.8106045126914978\n",
      "epoch =  14  step =  400  of total steps  1937  loss =  1.5655665397644043\n",
      "epoch =  14  step =  600  of total steps  1937  loss =  1.0365186929702759\n",
      "epoch =  14  step =  800  of total steps  1937  loss =  1.3354040384292603\n",
      "epoch =  14  step =  1000  of total steps  1937  loss =  1.0434688329696655\n",
      "epoch =  14  step =  1200  of total steps  1937  loss =  0.5690916180610657\n",
      "epoch =  14  step =  1400  of total steps  1937  loss =  0.902957022190094\n",
      "epoch =  14  step =  1600  of total steps  1937  loss =  0.8851710557937622\n",
      "epoch =  14  step =  1800  of total steps  1937  loss =  1.083504319190979\n",
      "epoch :  14  /  100  | TL :  1.0571917713180792  | VL :  1.03256094455719\n",
      "saving model\n",
      "epoch =  15  step =  0  of total steps  1937  loss =  0.47860634326934814\n",
      "epoch =  15  step =  200  of total steps  1937  loss =  1.2189770936965942\n",
      "epoch =  15  step =  400  of total steps  1937  loss =  0.6436982154846191\n",
      "epoch =  15  step =  600  of total steps  1937  loss =  0.9926900267601013\n",
      "epoch =  15  step =  800  of total steps  1937  loss =  0.8261255025863647\n",
      "epoch =  15  step =  1000  of total steps  1937  loss =  0.8391335010528564\n",
      "epoch =  15  step =  1200  of total steps  1937  loss =  0.7515170574188232\n",
      "epoch =  15  step =  1400  of total steps  1937  loss =  1.0410540103912354\n",
      "epoch =  15  step =  1600  of total steps  1937  loss =  1.5603671073913574\n",
      "epoch =  15  step =  1800  of total steps  1937  loss =  0.8814887404441833\n",
      "epoch :  15  /  100  | TL :  1.0538094649625058  | VL :  1.3186529874801636\n",
      "epoch =  16  step =  0  of total steps  1937  loss =  0.9429498910903931\n",
      "epoch =  16  step =  200  of total steps  1937  loss =  1.2742725610733032\n",
      "epoch =  16  step =  400  of total steps  1937  loss =  0.7967129945755005\n",
      "epoch =  16  step =  600  of total steps  1937  loss =  0.7350754737854004\n",
      "epoch =  16  step =  800  of total steps  1937  loss =  2.351170778274536\n",
      "epoch =  16  step =  1000  of total steps  1937  loss =  0.6587194800376892\n",
      "epoch =  16  step =  1200  of total steps  1937  loss =  1.0650943517684937\n",
      "epoch =  16  step =  1400  of total steps  1937  loss =  0.9991686344146729\n",
      "epoch =  16  step =  1600  of total steps  1937  loss =  1.2043089866638184\n",
      "epoch =  16  step =  1800  of total steps  1937  loss =  0.9118250012397766\n",
      "epoch :  16  /  100  | TL :  1.0505645007402797  | VL :  1.0613651275634766\n",
      "epoch =  17  step =  0  of total steps  1937  loss =  0.8179194331169128\n",
      "epoch =  17  step =  200  of total steps  1937  loss =  0.9810031652450562\n",
      "epoch =  17  step =  400  of total steps  1937  loss =  0.7344083786010742\n",
      "epoch =  17  step =  600  of total steps  1937  loss =  1.1680562496185303\n",
      "epoch =  17  step =  800  of total steps  1937  loss =  1.089660882949829\n",
      "epoch =  17  step =  1000  of total steps  1937  loss =  0.7624983787536621\n",
      "epoch =  17  step =  1200  of total steps  1937  loss =  1.418982982635498\n",
      "epoch =  17  step =  1400  of total steps  1937  loss =  0.843694806098938\n",
      "epoch =  17  step =  1600  of total steps  1937  loss =  1.5014519691467285\n",
      "epoch =  17  step =  1800  of total steps  1937  loss =  0.6896288394927979\n",
      "epoch :  17  /  100  | TL :  1.0364427983330031  | VL :  1.1840630769729614\n",
      "epoch =  18  step =  0  of total steps  1937  loss =  1.7847002744674683\n",
      "epoch =  18  step =  200  of total steps  1937  loss =  0.9931076765060425\n",
      "epoch =  18  step =  400  of total steps  1937  loss =  1.9576414823532104\n",
      "epoch =  18  step =  600  of total steps  1937  loss =  1.362330675125122\n",
      "epoch =  18  step =  800  of total steps  1937  loss =  1.0601106882095337\n",
      "epoch =  18  step =  1000  of total steps  1937  loss =  0.6290712356567383\n",
      "epoch =  18  step =  1200  of total steps  1937  loss =  1.1134414672851562\n",
      "epoch =  18  step =  1400  of total steps  1937  loss =  1.6631118059158325\n",
      "epoch =  18  step =  1600  of total steps  1937  loss =  0.8623033165931702\n",
      "epoch =  18  step =  1800  of total steps  1937  loss =  0.6917761564254761\n",
      "epoch :  18  /  100  | TL :  1.0331471282671454  | VL :  1.045084834098816\n",
      "epoch =  19  step =  0  of total steps  1937  loss =  1.1644010543823242\n",
      "epoch =  19  step =  200  of total steps  1937  loss =  0.9961411952972412\n",
      "epoch =  19  step =  400  of total steps  1937  loss =  0.4524451494216919\n",
      "epoch =  19  step =  600  of total steps  1937  loss =  1.0248435735702515\n",
      "epoch =  19  step =  800  of total steps  1937  loss =  1.2616984844207764\n",
      "epoch =  19  step =  1000  of total steps  1937  loss =  0.7487643957138062\n",
      "epoch =  19  step =  1200  of total steps  1937  loss =  1.3780134916305542\n",
      "epoch =  19  step =  1400  of total steps  1937  loss =  0.6969572305679321\n",
      "epoch =  19  step =  1600  of total steps  1937  loss =  1.1118749380111694\n",
      "epoch =  19  step =  1800  of total steps  1937  loss =  0.6385151743888855\n",
      "epoch :  19  /  100  | TL :  1.0231334765085438  | VL :  1.0636489391326904\n",
      "epoch =  20  step =  0  of total steps  1937  loss =  0.6756121516227722\n",
      "epoch =  20  step =  200  of total steps  1937  loss =  0.9826861023902893\n",
      "epoch =  20  step =  400  of total steps  1937  loss =  1.132294774055481\n",
      "epoch =  20  step =  600  of total steps  1937  loss =  0.874212384223938\n",
      "epoch =  20  step =  800  of total steps  1937  loss =  1.0177992582321167\n",
      "epoch =  20  step =  1000  of total steps  1937  loss =  1.20462965965271\n",
      "epoch =  20  step =  1200  of total steps  1937  loss =  1.0797501802444458\n",
      "epoch =  20  step =  1400  of total steps  1937  loss =  0.9796156883239746\n",
      "epoch =  20  step =  1600  of total steps  1937  loss =  0.8036027550697327\n",
      "epoch =  20  step =  1800  of total steps  1937  loss =  0.841154932975769\n",
      "epoch :  20  /  100  | TL :  1.0239168990809215  | VL :  1.0147285461425781\n",
      "saving model\n",
      "epoch =  21  step =  0  of total steps  1937  loss =  0.877331554889679\n",
      "epoch =  21  step =  200  of total steps  1937  loss =  0.588462769985199\n",
      "epoch =  21  step =  400  of total steps  1937  loss =  1.3773359060287476\n",
      "epoch =  21  step =  600  of total steps  1937  loss =  1.0968077182769775\n",
      "epoch =  21  step =  800  of total steps  1937  loss =  0.9078413248062134\n",
      "epoch =  21  step =  1000  of total steps  1937  loss =  1.6621161699295044\n",
      "epoch =  21  step =  1200  of total steps  1937  loss =  1.2071110010147095\n",
      "epoch =  21  step =  1400  of total steps  1937  loss =  1.1492230892181396\n",
      "epoch =  21  step =  1600  of total steps  1937  loss =  0.6800976395606995\n",
      "epoch =  21  step =  1800  of total steps  1937  loss =  0.8814923763275146\n",
      "epoch :  21  /  100  | TL :  1.0183196518152844  | VL :  1.0166473388671875\n",
      "epoch =  22  step =  0  of total steps  1937  loss =  0.8220037817955017\n",
      "epoch =  22  step =  200  of total steps  1937  loss =  0.93451327085495\n",
      "epoch =  22  step =  400  of total steps  1937  loss =  1.1602544784545898\n",
      "epoch =  22  step =  600  of total steps  1937  loss =  0.8911172747612\n",
      "epoch =  22  step =  800  of total steps  1937  loss =  1.0920743942260742\n",
      "epoch =  22  step =  1000  of total steps  1937  loss =  0.8031331300735474\n",
      "epoch =  22  step =  1200  of total steps  1937  loss =  0.6908642649650574\n",
      "epoch =  22  step =  1400  of total steps  1937  loss =  0.7317082285881042\n",
      "epoch =  22  step =  1600  of total steps  1937  loss =  1.4037870168685913\n",
      "epoch =  22  step =  1800  of total steps  1937  loss =  0.9896227717399597\n",
      "epoch :  22  /  100  | TL :  1.012270099872403  | VL :  1.0270200967788696\n",
      "epoch =  23  step =  0  of total steps  1937  loss =  0.777859091758728\n",
      "epoch =  23  step =  200  of total steps  1937  loss =  1.173111081123352\n",
      "epoch =  23  step =  400  of total steps  1937  loss =  1.208367109298706\n",
      "epoch =  23  step =  600  of total steps  1937  loss =  1.4422377347946167\n",
      "epoch =  23  step =  800  of total steps  1937  loss =  1.3003507852554321\n",
      "epoch =  23  step =  1000  of total steps  1937  loss =  1.3906245231628418\n",
      "epoch =  23  step =  1200  of total steps  1937  loss =  0.637676477432251\n",
      "epoch =  23  step =  1400  of total steps  1937  loss =  0.8028500080108643\n",
      "epoch =  23  step =  1600  of total steps  1937  loss =  1.0415202379226685\n",
      "epoch =  23  step =  1800  of total steps  1937  loss =  1.6122862100601196\n",
      "epoch :  23  /  100  | TL :  1.0062865293635248  | VL :  1.0139739513397217\n",
      "saving model\n",
      "epoch =  24  step =  0  of total steps  1937  loss =  1.3233616352081299\n",
      "epoch =  24  step =  200  of total steps  1937  loss =  0.9666609764099121\n",
      "epoch =  24  step =  400  of total steps  1937  loss =  1.0222817659378052\n",
      "epoch =  24  step =  600  of total steps  1937  loss =  1.4524686336517334\n",
      "epoch =  24  step =  800  of total steps  1937  loss =  0.9950206279754639\n",
      "epoch =  24  step =  1000  of total steps  1937  loss =  1.184471607208252\n",
      "epoch =  24  step =  1200  of total steps  1937  loss =  1.012349247932434\n",
      "epoch =  24  step =  1400  of total steps  1937  loss =  1.7275663614273071\n",
      "epoch =  24  step =  1600  of total steps  1937  loss =  0.6965373158454895\n",
      "epoch =  24  step =  1800  of total steps  1937  loss =  1.1737968921661377\n",
      "epoch :  24  /  100  | TL :  0.9984574125917166  | VL :  1.0268570184707642\n",
      "epoch =  25  step =  0  of total steps  1937  loss =  0.9104546904563904\n",
      "epoch =  25  step =  200  of total steps  1937  loss =  0.7931251525878906\n",
      "epoch =  25  step =  400  of total steps  1937  loss =  1.1608105897903442\n",
      "epoch =  25  step =  600  of total steps  1937  loss =  1.087845802307129\n",
      "epoch =  25  step =  800  of total steps  1937  loss =  1.2395416498184204\n",
      "epoch =  25  step =  1000  of total steps  1937  loss =  0.7427946925163269\n",
      "epoch =  25  step =  1200  of total steps  1937  loss =  1.5121039152145386\n",
      "epoch =  25  step =  1400  of total steps  1937  loss =  0.6206852793693542\n",
      "epoch =  25  step =  1600  of total steps  1937  loss =  0.5149049758911133\n",
      "epoch =  25  step =  1800  of total steps  1937  loss =  1.5540010929107666\n",
      "epoch :  25  /  100  | TL :  0.9937270834665047  | VL :  1.0328986644744873\n",
      "epoch =  26  step =  0  of total steps  1937  loss =  1.3826221227645874\n",
      "epoch =  26  step =  200  of total steps  1937  loss =  0.8544752597808838\n",
      "epoch =  26  step =  400  of total steps  1937  loss =  0.5288066864013672\n",
      "epoch =  26  step =  600  of total steps  1937  loss =  0.9976293444633484\n",
      "epoch =  26  step =  800  of total steps  1937  loss =  1.085634708404541\n",
      "epoch =  26  step =  1000  of total steps  1937  loss =  1.5391688346862793\n",
      "epoch =  26  step =  1200  of total steps  1937  loss =  0.873241126537323\n",
      "epoch =  26  step =  1400  of total steps  1937  loss =  1.3535301685333252\n",
      "epoch =  26  step =  1600  of total steps  1937  loss =  0.6122345924377441\n",
      "epoch =  26  step =  1800  of total steps  1937  loss =  0.9774981141090393\n",
      "epoch :  26  /  100  | TL :  0.9913182953121616  | VL :  1.0382459163665771\n",
      "epoch =  27  step =  0  of total steps  1937  loss =  0.9981556534767151\n",
      "epoch =  27  step =  200  of total steps  1937  loss =  0.8443788886070251\n",
      "epoch =  27  step =  400  of total steps  1937  loss =  1.1714047193527222\n",
      "epoch =  27  step =  600  of total steps  1937  loss =  0.5253244638442993\n",
      "epoch =  27  step =  800  of total steps  1937  loss =  0.8772891759872437\n",
      "epoch =  27  step =  1000  of total steps  1937  loss =  0.8246272206306458\n",
      "epoch =  27  step =  1200  of total steps  1937  loss =  0.7859737873077393\n",
      "epoch =  27  step =  1400  of total steps  1937  loss =  1.1726157665252686\n",
      "epoch =  27  step =  1600  of total steps  1937  loss =  1.7001794576644897\n",
      "epoch =  27  step =  1800  of total steps  1937  loss =  0.8401647806167603\n",
      "epoch :  27  /  100  | TL :  0.9911492609706932  | VL :  0.9744487404823303\n",
      "saving model\n",
      "epoch =  28  step =  0  of total steps  1937  loss =  1.112441062927246\n",
      "epoch =  28  step =  200  of total steps  1937  loss =  0.5843031406402588\n",
      "epoch =  28  step =  400  of total steps  1937  loss =  1.570277452468872\n",
      "epoch =  28  step =  600  of total steps  1937  loss =  1.5560534000396729\n",
      "epoch =  28  step =  800  of total steps  1937  loss =  1.0838993787765503\n",
      "epoch =  28  step =  1000  of total steps  1937  loss =  1.1782854795455933\n",
      "epoch =  28  step =  1200  of total steps  1937  loss =  0.9677009582519531\n",
      "epoch =  28  step =  1400  of total steps  1937  loss =  0.8168455362319946\n",
      "epoch =  28  step =  1600  of total steps  1937  loss =  0.7137668132781982\n",
      "epoch =  28  step =  1800  of total steps  1937  loss =  0.9157712459564209\n",
      "epoch :  28  /  100  | TL :  0.9853825108732338  | VL :  0.9788945317268372\n",
      "epoch =  29  step =  0  of total steps  1937  loss =  1.3743070363998413\n",
      "epoch =  29  step =  200  of total steps  1937  loss =  0.6150106191635132\n",
      "epoch =  29  step =  400  of total steps  1937  loss =  1.331950306892395\n",
      "epoch =  29  step =  600  of total steps  1937  loss =  1.0566153526306152\n",
      "epoch =  29  step =  800  of total steps  1937  loss =  1.0837218761444092\n",
      "epoch =  29  step =  1000  of total steps  1937  loss =  1.091282606124878\n",
      "epoch =  29  step =  1200  of total steps  1937  loss =  1.2412433624267578\n",
      "epoch =  29  step =  1400  of total steps  1937  loss =  0.5611114501953125\n",
      "epoch =  29  step =  1600  of total steps  1937  loss =  0.8751676082611084\n",
      "epoch =  29  step =  1800  of total steps  1937  loss =  1.2643013000488281\n",
      "epoch :  29  /  100  | TL :  0.9745391997161081  | VL :  1.0689254999160767\n",
      "epoch =  30  step =  0  of total steps  1937  loss =  0.7357024550437927\n",
      "epoch =  30  step =  200  of total steps  1937  loss =  1.3767179250717163\n",
      "epoch =  30  step =  400  of total steps  1937  loss =  0.8043164610862732\n",
      "epoch =  30  step =  600  of total steps  1937  loss =  0.49327585101127625\n",
      "epoch =  30  step =  800  of total steps  1937  loss =  1.175702452659607\n",
      "epoch =  30  step =  1000  of total steps  1937  loss =  0.7982291579246521\n",
      "epoch =  30  step =  1200  of total steps  1937  loss =  0.746901273727417\n",
      "epoch =  30  step =  1400  of total steps  1937  loss =  1.1206858158111572\n",
      "epoch =  30  step =  1600  of total steps  1937  loss =  1.0259987115859985\n",
      "epoch =  30  step =  1800  of total steps  1937  loss =  0.7404690980911255\n",
      "epoch :  30  /  100  | TL :  0.9762173031154341  | VL :  0.9763274192810059\n",
      "epoch =  31  step =  0  of total steps  1937  loss =  0.7188764214515686\n",
      "epoch =  31  step =  200  of total steps  1937  loss =  1.0985188484191895\n",
      "epoch =  31  step =  400  of total steps  1937  loss =  1.1143181324005127\n",
      "epoch =  31  step =  600  of total steps  1937  loss =  1.191904902458191\n",
      "epoch =  31  step =  800  of total steps  1937  loss =  1.0931038856506348\n",
      "epoch =  31  step =  1000  of total steps  1937  loss =  0.975875735282898\n",
      "epoch =  31  step =  1200  of total steps  1937  loss =  0.6606014966964722\n",
      "epoch =  31  step =  1400  of total steps  1937  loss =  0.7470759749412537\n",
      "epoch =  31  step =  1600  of total steps  1937  loss =  1.147214412689209\n",
      "epoch =  31  step =  1800  of total steps  1937  loss =  0.2635810673236847\n",
      "epoch :  31  /  100  | TL :  0.9687472266761925  | VL :  0.9871751070022583\n",
      "epoch =  32  step =  0  of total steps  1937  loss =  1.2303293943405151\n",
      "epoch =  32  step =  200  of total steps  1937  loss =  0.7023531794548035\n",
      "epoch =  32  step =  400  of total steps  1937  loss =  0.678032398223877\n",
      "epoch =  32  step =  600  of total steps  1937  loss =  1.5974127054214478\n",
      "epoch =  32  step =  800  of total steps  1937  loss =  0.9523513913154602\n",
      "epoch =  32  step =  1000  of total steps  1937  loss =  0.6544785499572754\n",
      "epoch =  32  step =  1200  of total steps  1937  loss =  1.0255717039108276\n",
      "epoch =  32  step =  1400  of total steps  1937  loss =  1.5828006267547607\n",
      "epoch =  32  step =  1600  of total steps  1937  loss =  1.4236754179000854\n",
      "epoch =  32  step =  1800  of total steps  1937  loss =  0.928268313407898\n",
      "epoch :  32  /  100  | TL :  0.9658681276414465  | VL :  0.9955217838287354\n",
      "epoch =  33  step =  0  of total steps  1937  loss =  1.212462067604065\n",
      "epoch =  33  step =  200  of total steps  1937  loss =  0.83219313621521\n",
      "epoch =  33  step =  400  of total steps  1937  loss =  0.6166372895240784\n",
      "epoch =  33  step =  600  of total steps  1937  loss =  1.224679708480835\n",
      "epoch =  33  step =  800  of total steps  1937  loss =  1.1003937721252441\n",
      "epoch =  33  step =  1000  of total steps  1937  loss =  0.9153091311454773\n",
      "epoch =  33  step =  1200  of total steps  1937  loss =  0.4678780138492584\n",
      "epoch =  33  step =  1400  of total steps  1937  loss =  1.5088387727737427\n",
      "epoch =  33  step =  1600  of total steps  1937  loss =  1.0195170640945435\n",
      "epoch =  33  step =  1800  of total steps  1937  loss =  0.6838692426681519\n",
      "epoch :  33  /  100  | TL :  0.9617763717702571  | VL :  0.9622097611427307\n",
      "saving model\n",
      "epoch =  34  step =  0  of total steps  1937  loss =  1.3857533931732178\n",
      "epoch =  34  step =  200  of total steps  1937  loss =  0.7782903909683228\n",
      "epoch =  34  step =  400  of total steps  1937  loss =  1.026766300201416\n",
      "epoch =  34  step =  600  of total steps  1937  loss =  0.744575023651123\n",
      "epoch =  34  step =  800  of total steps  1937  loss =  1.0618165731430054\n",
      "epoch =  34  step =  1000  of total steps  1937  loss =  1.2224382162094116\n",
      "epoch =  34  step =  1200  of total steps  1937  loss =  1.0997790098190308\n",
      "epoch =  34  step =  1400  of total steps  1937  loss =  0.6447349190711975\n",
      "epoch =  34  step =  1600  of total steps  1937  loss =  1.2893996238708496\n",
      "epoch =  34  step =  1800  of total steps  1937  loss =  0.9676171541213989\n",
      "epoch :  34  /  100  | TL :  0.9686352615742322  | VL :  0.9818336963653564\n",
      "epoch =  35  step =  0  of total steps  1937  loss =  1.804805874824524\n",
      "epoch =  35  step =  200  of total steps  1937  loss =  0.5327016711235046\n",
      "epoch =  35  step =  400  of total steps  1937  loss =  1.2339237928390503\n",
      "epoch =  35  step =  600  of total steps  1937  loss =  0.8656359314918518\n",
      "epoch =  35  step =  800  of total steps  1937  loss =  0.9322031736373901\n",
      "epoch =  35  step =  1000  of total steps  1937  loss =  1.5997254848480225\n",
      "epoch =  35  step =  1200  of total steps  1937  loss =  0.935211181640625\n",
      "epoch =  35  step =  1400  of total steps  1937  loss =  1.0536773204803467\n",
      "epoch =  35  step =  1600  of total steps  1937  loss =  1.252614140510559\n",
      "epoch =  35  step =  1800  of total steps  1937  loss =  1.1056392192840576\n",
      "epoch :  35  /  100  | TL :  0.9660127660664444  | VL :  0.9695034027099609\n",
      "epoch =  36  step =  0  of total steps  1937  loss =  0.7937344312667847\n",
      "epoch =  36  step =  200  of total steps  1937  loss =  1.1060497760772705\n",
      "epoch =  36  step =  400  of total steps  1937  loss =  1.0988385677337646\n",
      "epoch =  36  step =  600  of total steps  1937  loss =  0.8498176336288452\n",
      "epoch =  36  step =  800  of total steps  1937  loss =  0.8547133207321167\n",
      "epoch =  36  step =  1000  of total steps  1937  loss =  1.3864003419876099\n",
      "epoch =  36  step =  1200  of total steps  1937  loss =  0.7299896478652954\n",
      "epoch =  36  step =  1400  of total steps  1937  loss =  0.8946723937988281\n",
      "epoch =  36  step =  1600  of total steps  1937  loss =  1.2875810861587524\n",
      "epoch =  36  step =  1800  of total steps  1937  loss =  0.7555403113365173\n",
      "epoch :  36  /  100  | TL :  0.9581706893770241  | VL :  0.998853325843811\n",
      "epoch =  37  step =  0  of total steps  1937  loss =  1.0664476156234741\n",
      "epoch =  37  step =  200  of total steps  1937  loss =  0.5393909215927124\n",
      "epoch =  37  step =  400  of total steps  1937  loss =  0.34633272886276245\n",
      "epoch =  37  step =  600  of total steps  1937  loss =  0.6447994709014893\n",
      "epoch =  37  step =  800  of total steps  1937  loss =  0.872209906578064\n",
      "epoch =  37  step =  1000  of total steps  1937  loss =  0.8021634817123413\n",
      "epoch =  37  step =  1200  of total steps  1937  loss =  0.7484692931175232\n",
      "epoch =  37  step =  1400  of total steps  1937  loss =  0.8852711319923401\n",
      "epoch =  37  step =  1600  of total steps  1937  loss =  0.7147775888442993\n",
      "epoch =  37  step =  1800  of total steps  1937  loss =  0.5073453187942505\n",
      "epoch :  37  /  100  | TL :  0.9516214223546839  | VL :  0.9790421724319458\n",
      "epoch =  38  step =  0  of total steps  1937  loss =  0.4990716278553009\n",
      "epoch =  38  step =  200  of total steps  1937  loss =  1.0588653087615967\n",
      "epoch =  38  step =  400  of total steps  1937  loss =  0.8728865385055542\n",
      "epoch =  38  step =  600  of total steps  1937  loss =  1.6333941221237183\n",
      "epoch =  38  step =  800  of total steps  1937  loss =  1.5046489238739014\n",
      "epoch =  38  step =  1000  of total steps  1937  loss =  0.49429255723953247\n",
      "epoch =  38  step =  1200  of total steps  1937  loss =  0.8621055483818054\n",
      "epoch =  38  step =  1400  of total steps  1937  loss =  0.9049431681632996\n",
      "epoch =  38  step =  1600  of total steps  1937  loss =  1.151312232017517\n",
      "epoch =  38  step =  1800  of total steps  1937  loss =  0.9701576232910156\n",
      "epoch :  38  /  100  | TL :  0.9514510227222078  | VL :  1.0348734855651855\n",
      "epoch =  39  step =  0  of total steps  1937  loss =  1.5700798034667969\n",
      "epoch =  39  step =  200  of total steps  1937  loss =  1.1577398777008057\n",
      "epoch =  39  step =  400  of total steps  1937  loss =  0.8446428179740906\n",
      "epoch =  39  step =  600  of total steps  1937  loss =  0.5899103879928589\n",
      "epoch =  39  step =  800  of total steps  1937  loss =  0.7982476949691772\n",
      "epoch =  39  step =  1000  of total steps  1937  loss =  1.217835783958435\n",
      "epoch =  39  step =  1200  of total steps  1937  loss =  1.672821044921875\n",
      "epoch =  39  step =  1400  of total steps  1937  loss =  0.6788353323936462\n",
      "epoch =  39  step =  1600  of total steps  1937  loss =  1.53306245803833\n",
      "epoch =  39  step =  1800  of total steps  1937  loss =  1.073130488395691\n",
      "epoch :  39  /  100  | TL :  0.9451848292279994  | VL :  0.9692522287368774\n",
      "epoch =  40  step =  0  of total steps  1937  loss =  0.7606664299964905\n",
      "epoch =  40  step =  200  of total steps  1937  loss =  1.4382615089416504\n",
      "epoch =  40  step =  400  of total steps  1937  loss =  0.6541011333465576\n",
      "epoch =  40  step =  600  of total steps  1937  loss =  1.1052031517028809\n",
      "epoch =  40  step =  800  of total steps  1937  loss =  1.4493898153305054\n",
      "epoch =  40  step =  1000  of total steps  1937  loss =  0.6279783248901367\n",
      "epoch =  40  step =  1200  of total steps  1937  loss =  0.9612795114517212\n",
      "epoch =  40  step =  1400  of total steps  1937  loss =  0.5054233074188232\n",
      "epoch =  40  step =  1600  of total steps  1937  loss =  1.097919225692749\n",
      "epoch =  40  step =  1800  of total steps  1937  loss =  0.9464561343193054\n",
      "epoch :  40  /  100  | TL :  0.9453439839303525  | VL :  0.9697638750076294\n",
      "epoch =  41  step =  0  of total steps  1937  loss =  1.112053394317627\n",
      "epoch =  41  step =  200  of total steps  1937  loss =  0.8465558290481567\n",
      "epoch =  41  step =  400  of total steps  1937  loss =  0.9999499917030334\n",
      "epoch =  41  step =  600  of total steps  1937  loss =  0.773155927658081\n",
      "epoch =  41  step =  800  of total steps  1937  loss =  0.8648387789726257\n",
      "epoch =  41  step =  1000  of total steps  1937  loss =  0.5023607611656189\n",
      "epoch =  41  step =  1200  of total steps  1937  loss =  1.4500375986099243\n",
      "epoch =  41  step =  1400  of total steps  1937  loss =  1.3912776708602905\n",
      "epoch =  41  step =  1600  of total steps  1937  loss =  0.5964464545249939\n",
      "epoch =  41  step =  1800  of total steps  1937  loss =  1.458727240562439\n",
      "epoch :  41  /  100  | TL :  0.9402624489138529  | VL :  0.965394914150238\n",
      "epoch =  42  step =  0  of total steps  1937  loss =  1.055271029472351\n",
      "epoch =  42  step =  200  of total steps  1937  loss =  0.7429584860801697\n",
      "epoch =  42  step =  400  of total steps  1937  loss =  0.8136618733406067\n",
      "epoch =  42  step =  600  of total steps  1937  loss =  1.7313361167907715\n",
      "epoch =  42  step =  800  of total steps  1937  loss =  1.2856464385986328\n",
      "epoch =  42  step =  1000  of total steps  1937  loss =  0.6647943258285522\n",
      "epoch =  42  step =  1200  of total steps  1937  loss =  0.5757136940956116\n",
      "epoch =  42  step =  1400  of total steps  1937  loss =  1.0243823528289795\n",
      "epoch =  42  step =  1600  of total steps  1937  loss =  0.2650064527988434\n",
      "epoch =  42  step =  1800  of total steps  1937  loss =  0.94533371925354\n",
      "epoch :  42  /  100  | TL :  0.9417321707670628  | VL :  1.002159595489502\n",
      "epoch =  43  step =  0  of total steps  1937  loss =  0.9652963280677795\n",
      "epoch =  43  step =  200  of total steps  1937  loss =  0.8543472290039062\n",
      "epoch =  43  step =  400  of total steps  1937  loss =  1.0336118936538696\n",
      "epoch =  43  step =  600  of total steps  1937  loss =  0.9393426775932312\n",
      "epoch =  43  step =  800  of total steps  1937  loss =  0.8931390643119812\n",
      "epoch =  43  step =  1000  of total steps  1937  loss =  1.0861104726791382\n",
      "epoch =  43  step =  1200  of total steps  1937  loss =  0.4825993478298187\n",
      "epoch =  43  step =  1400  of total steps  1937  loss =  0.47316449880599976\n",
      "epoch =  43  step =  1600  of total steps  1937  loss =  0.6230201125144958\n",
      "epoch =  43  step =  1800  of total steps  1937  loss =  0.4793311357498169\n",
      "epoch :  43  /  100  | TL :  0.940991456476634  | VL :  1.011044979095459\n",
      "epoch =  44  step =  0  of total steps  1937  loss =  0.9051926732063293\n",
      "epoch =  44  step =  200  of total steps  1937  loss =  1.1357121467590332\n",
      "epoch =  44  step =  400  of total steps  1937  loss =  1.2933183908462524\n",
      "epoch =  44  step =  600  of total steps  1937  loss =  1.1785008907318115\n",
      "epoch =  44  step =  800  of total steps  1937  loss =  0.8557306528091431\n",
      "epoch =  44  step =  1000  of total steps  1937  loss =  0.8053495287895203\n",
      "epoch =  44  step =  1200  of total steps  1937  loss =  0.9183083176612854\n",
      "epoch =  44  step =  1400  of total steps  1937  loss =  0.8624280095100403\n",
      "epoch =  44  step =  1600  of total steps  1937  loss =  0.4304233491420746\n",
      "epoch =  44  step =  1800  of total steps  1937  loss =  1.4144477844238281\n",
      "epoch :  44  /  100  | TL :  0.9322258332339401  | VL :  0.9395392537117004\n",
      "saving model\n",
      "epoch =  45  step =  0  of total steps  1937  loss =  0.4888797104358673\n",
      "epoch =  45  step =  200  of total steps  1937  loss =  0.8248299956321716\n",
      "epoch =  45  step =  400  of total steps  1937  loss =  0.6718332767486572\n",
      "epoch =  45  step =  600  of total steps  1937  loss =  0.40290188789367676\n",
      "epoch =  45  step =  800  of total steps  1937  loss =  0.95745849609375\n",
      "epoch =  45  step =  1000  of total steps  1937  loss =  1.2967593669891357\n",
      "epoch =  45  step =  1200  of total steps  1937  loss =  0.75725257396698\n",
      "epoch =  45  step =  1400  of total steps  1937  loss =  1.0515239238739014\n",
      "epoch =  45  step =  1600  of total steps  1937  loss =  0.8450738787651062\n",
      "epoch =  45  step =  1800  of total steps  1937  loss =  0.6085340976715088\n",
      "epoch :  45  /  100  | TL :  0.9269107715887819  | VL :  0.9421595931053162\n",
      "epoch =  46  step =  0  of total steps  1937  loss =  0.6971861124038696\n",
      "epoch =  46  step =  200  of total steps  1937  loss =  1.4096271991729736\n",
      "epoch =  46  step =  400  of total steps  1937  loss =  0.43913158774375916\n",
      "epoch =  46  step =  600  of total steps  1937  loss =  0.9262523055076599\n",
      "epoch =  46  step =  800  of total steps  1937  loss =  1.3187564611434937\n",
      "epoch =  46  step =  1000  of total steps  1937  loss =  1.0618559122085571\n",
      "epoch =  46  step =  1200  of total steps  1937  loss =  0.6079445481300354\n",
      "epoch =  46  step =  1400  of total steps  1937  loss =  1.1105842590332031\n",
      "epoch =  46  step =  1600  of total steps  1937  loss =  1.3278486728668213\n",
      "epoch =  46  step =  1800  of total steps  1937  loss =  1.0027052164077759\n",
      "epoch :  46  /  100  | TL :  0.9269071559531823  | VL :  0.9449092745780945\n",
      "epoch =  47  step =  0  of total steps  1937  loss =  0.49959421157836914\n",
      "epoch =  47  step =  200  of total steps  1937  loss =  0.9254631400108337\n",
      "epoch =  47  step =  400  of total steps  1937  loss =  0.7727808356285095\n",
      "epoch =  47  step =  600  of total steps  1937  loss =  0.8336830735206604\n",
      "epoch =  47  step =  800  of total steps  1937  loss =  0.7836732864379883\n",
      "epoch =  47  step =  1000  of total steps  1937  loss =  0.8027175068855286\n",
      "epoch =  47  step =  1200  of total steps  1937  loss =  1.1254488229751587\n",
      "epoch =  47  step =  1400  of total steps  1937  loss =  0.6661041378974915\n",
      "epoch =  47  step =  1600  of total steps  1937  loss =  0.8455736637115479\n",
      "epoch =  47  step =  1800  of total steps  1937  loss =  0.7390896677970886\n",
      "epoch :  47  /  100  | TL :  0.9208166839523173  | VL :  0.9667267203330994\n",
      "epoch =  48  step =  0  of total steps  1937  loss =  1.3237488269805908\n",
      "epoch =  48  step =  200  of total steps  1937  loss =  1.121212363243103\n",
      "epoch =  48  step =  400  of total steps  1937  loss =  0.9189882278442383\n",
      "epoch =  48  step =  600  of total steps  1937  loss =  0.8233785629272461\n",
      "epoch =  48  step =  800  of total steps  1937  loss =  0.8355057239532471\n",
      "epoch =  48  step =  1000  of total steps  1937  loss =  0.8709015846252441\n",
      "epoch =  48  step =  1200  of total steps  1937  loss =  1.045961856842041\n",
      "epoch =  48  step =  1400  of total steps  1937  loss =  1.0427159070968628\n",
      "epoch =  48  step =  1600  of total steps  1937  loss =  0.700821578502655\n",
      "epoch =  48  step =  1800  of total steps  1937  loss =  0.7264760136604309\n",
      "epoch :  48  /  100  | TL :  0.9213304262325576  | VL :  1.0030038356781006\n",
      "epoch =  49  step =  0  of total steps  1937  loss =  0.9733654260635376\n",
      "epoch =  49  step =  200  of total steps  1937  loss =  0.8612347841262817\n",
      "epoch =  49  step =  400  of total steps  1937  loss =  0.6992930173873901\n",
      "epoch =  49  step =  600  of total steps  1937  loss =  1.2117892503738403\n",
      "epoch =  49  step =  800  of total steps  1937  loss =  1.3778225183486938\n",
      "epoch =  49  step =  1000  of total steps  1937  loss =  1.0918830633163452\n",
      "epoch =  49  step =  1200  of total steps  1937  loss =  0.713910698890686\n",
      "epoch =  49  step =  1400  of total steps  1937  loss =  0.5006102323532104\n",
      "epoch =  49  step =  1600  of total steps  1937  loss =  0.561933159828186\n",
      "epoch =  49  step =  1800  of total steps  1937  loss =  1.0147514343261719\n",
      "epoch :  49  /  100  | TL :  0.9213429598631044  | VL :  0.959164559841156\n",
      "epoch =  50  step =  0  of total steps  1937  loss =  1.074765682220459\n",
      "epoch =  50  step =  200  of total steps  1937  loss =  1.0147405862808228\n",
      "epoch =  50  step =  400  of total steps  1937  loss =  0.5307416915893555\n",
      "epoch =  50  step =  600  of total steps  1937  loss =  0.6134133338928223\n",
      "epoch =  50  step =  800  of total steps  1937  loss =  0.7953643798828125\n",
      "epoch =  50  step =  1000  of total steps  1937  loss =  0.8773621320724487\n",
      "epoch =  50  step =  1200  of total steps  1937  loss =  0.8662972450256348\n",
      "epoch =  50  step =  1400  of total steps  1937  loss =  0.7841145396232605\n",
      "epoch =  50  step =  1600  of total steps  1937  loss =  0.7892959713935852\n",
      "epoch =  50  step =  1800  of total steps  1937  loss =  0.7008792161941528\n",
      "epoch :  50  /  100  | TL :  0.9185285840566219  | VL :  0.9813299775123596\n",
      "epoch =  51  step =  0  of total steps  1937  loss =  1.1182975769042969\n",
      "epoch =  51  step =  200  of total steps  1937  loss =  0.6024112105369568\n",
      "epoch =  51  step =  400  of total steps  1937  loss =  0.9687952995300293\n",
      "epoch =  51  step =  600  of total steps  1937  loss =  0.9398568868637085\n",
      "epoch =  51  step =  800  of total steps  1937  loss =  1.0863615274429321\n",
      "epoch =  51  step =  1000  of total steps  1937  loss =  0.43380892276763916\n",
      "epoch =  51  step =  1200  of total steps  1937  loss =  0.6414486765861511\n",
      "epoch =  51  step =  1400  of total steps  1937  loss =  2.0104706287384033\n",
      "epoch =  51  step =  1600  of total steps  1937  loss =  0.592617392539978\n",
      "epoch =  51  step =  1800  of total steps  1937  loss =  0.5932736396789551\n",
      "epoch :  51  /  100  | TL :  0.9154531261895696  | VL :  0.9380645155906677\n",
      "saving model\n",
      "epoch =  52  step =  0  of total steps  1937  loss =  1.2548173666000366\n",
      "epoch =  52  step =  200  of total steps  1937  loss =  0.6657382249832153\n",
      "epoch =  52  step =  400  of total steps  1937  loss =  0.8040575981140137\n",
      "epoch =  52  step =  600  of total steps  1937  loss =  0.8289650678634644\n",
      "epoch =  52  step =  800  of total steps  1937  loss =  1.1644762754440308\n",
      "epoch =  52  step =  1000  of total steps  1937  loss =  0.7426652312278748\n",
      "epoch =  52  step =  1200  of total steps  1937  loss =  0.9395090341567993\n",
      "epoch =  52  step =  1400  of total steps  1937  loss =  0.9856582283973694\n",
      "epoch =  52  step =  1600  of total steps  1937  loss =  0.6718170046806335\n",
      "epoch =  52  step =  1800  of total steps  1937  loss =  0.801121711730957\n",
      "epoch :  52  /  100  | TL :  0.9105579955373527  | VL :  0.9472975134849548\n",
      "epoch =  53  step =  0  of total steps  1937  loss =  0.8524994850158691\n",
      "epoch =  53  step =  200  of total steps  1937  loss =  0.8627963662147522\n",
      "epoch =  53  step =  400  of total steps  1937  loss =  0.8690271973609924\n",
      "epoch =  53  step =  600  of total steps  1937  loss =  1.1944303512573242\n",
      "epoch =  53  step =  800  of total steps  1937  loss =  1.0528732538223267\n",
      "epoch =  53  step =  1000  of total steps  1937  loss =  0.7556456327438354\n",
      "epoch =  53  step =  1200  of total steps  1937  loss =  1.0349724292755127\n",
      "epoch =  53  step =  1400  of total steps  1937  loss =  1.218001365661621\n",
      "epoch =  53  step =  1600  of total steps  1937  loss =  0.9952462911605835\n",
      "epoch =  53  step =  1800  of total steps  1937  loss =  1.1420742273330688\n",
      "epoch :  53  /  100  | TL :  0.912582576836454  | VL :  0.9724006652832031\n",
      "epoch =  54  step =  0  of total steps  1937  loss =  0.4382588565349579\n",
      "epoch =  54  step =  200  of total steps  1937  loss =  1.0325394868850708\n",
      "epoch =  54  step =  400  of total steps  1937  loss =  0.960811197757721\n",
      "epoch =  54  step =  600  of total steps  1937  loss =  0.49646511673927307\n",
      "epoch =  54  step =  800  of total steps  1937  loss =  1.8664085865020752\n",
      "epoch =  54  step =  1000  of total steps  1937  loss =  1.4886598587036133\n",
      "epoch =  54  step =  1200  of total steps  1937  loss =  0.5881922841072083\n",
      "epoch =  54  step =  1400  of total steps  1937  loss =  0.6596683263778687\n",
      "epoch =  54  step =  1600  of total steps  1937  loss =  0.3968455195426941\n",
      "epoch =  54  step =  1800  of total steps  1937  loss =  1.7286821603775024\n",
      "epoch :  54  /  100  | TL :  0.909934425467859  | VL :  0.9220341444015503\n",
      "saving model\n",
      "epoch =  55  step =  0  of total steps  1937  loss =  0.631933867931366\n",
      "epoch =  55  step =  200  of total steps  1937  loss =  0.9707065224647522\n",
      "epoch =  55  step =  400  of total steps  1937  loss =  0.9444559216499329\n",
      "epoch =  55  step =  600  of total steps  1937  loss =  0.6895705461502075\n",
      "epoch =  55  step =  800  of total steps  1937  loss =  0.3643176257610321\n",
      "epoch =  55  step =  1000  of total steps  1937  loss =  0.6372643709182739\n",
      "epoch =  55  step =  1200  of total steps  1937  loss =  1.5541143417358398\n",
      "epoch =  55  step =  1400  of total steps  1937  loss =  0.7786539196968079\n",
      "epoch =  55  step =  1600  of total steps  1937  loss =  0.7451552152633667\n",
      "epoch =  55  step =  1800  of total steps  1937  loss =  0.7311449646949768\n",
      "epoch :  55  /  100  | TL :  0.903785054369507  | VL :  0.9096142649650574\n",
      "saving model\n",
      "epoch =  56  step =  0  of total steps  1937  loss =  0.6655043363571167\n",
      "epoch =  56  step =  200  of total steps  1937  loss =  0.45875999331474304\n",
      "epoch =  56  step =  400  of total steps  1937  loss =  0.8929939866065979\n",
      "epoch =  56  step =  600  of total steps  1937  loss =  0.870428204536438\n",
      "epoch =  56  step =  800  of total steps  1937  loss =  1.3755455017089844\n",
      "epoch =  56  step =  1000  of total steps  1937  loss =  0.7420302629470825\n",
      "epoch =  56  step =  1200  of total steps  1937  loss =  1.0410081148147583\n",
      "epoch =  56  step =  1400  of total steps  1937  loss =  1.1003366708755493\n",
      "epoch =  56  step =  1600  of total steps  1937  loss =  0.3892882168292999\n",
      "epoch =  56  step =  1800  of total steps  1937  loss =  1.2230119705200195\n",
      "epoch :  56  /  100  | TL :  0.9054431564794352  | VL :  0.9678942561149597\n",
      "epoch =  57  step =  0  of total steps  1937  loss =  0.5256597995758057\n",
      "epoch =  57  step =  200  of total steps  1937  loss =  0.36804044246673584\n",
      "epoch =  57  step =  400  of total steps  1937  loss =  0.8295315504074097\n",
      "epoch =  57  step =  600  of total steps  1937  loss =  0.763961136341095\n",
      "epoch =  57  step =  800  of total steps  1937  loss =  0.8486592769622803\n",
      "epoch =  57  step =  1000  of total steps  1937  loss =  1.0507947206497192\n",
      "epoch =  57  step =  1200  of total steps  1937  loss =  0.8032400608062744\n",
      "epoch =  57  step =  1400  of total steps  1937  loss =  0.9891687631607056\n",
      "epoch =  57  step =  1600  of total steps  1937  loss =  1.0130823850631714\n",
      "epoch =  57  step =  1800  of total steps  1937  loss =  0.8891296982765198\n",
      "epoch :  57  /  100  | TL :  0.9018492291176141  | VL :  0.9337148070335388\n",
      "epoch =  58  step =  0  of total steps  1937  loss =  1.0413590669631958\n",
      "epoch =  58  step =  200  of total steps  1937  loss =  1.339093804359436\n",
      "epoch =  58  step =  400  of total steps  1937  loss =  1.166727900505066\n",
      "epoch =  58  step =  600  of total steps  1937  loss =  0.8865196108818054\n",
      "epoch =  58  step =  800  of total steps  1937  loss =  1.270512342453003\n",
      "epoch =  58  step =  1000  of total steps  1937  loss =  0.8630847930908203\n",
      "epoch =  58  step =  1200  of total steps  1937  loss =  1.3764528036117554\n",
      "epoch =  58  step =  1400  of total steps  1937  loss =  1.160223126411438\n",
      "epoch =  58  step =  1600  of total steps  1937  loss =  1.5680134296417236\n",
      "epoch =  58  step =  1800  of total steps  1937  loss =  0.7107213735580444\n",
      "epoch :  58  /  100  | TL :  0.9053214827363  | VL :  0.9334876537322998\n",
      "epoch =  59  step =  0  of total steps  1937  loss =  0.7461063861846924\n",
      "epoch =  59  step =  200  of total steps  1937  loss =  0.5734894275665283\n",
      "epoch =  59  step =  400  of total steps  1937  loss =  0.6620292067527771\n",
      "epoch =  59  step =  600  of total steps  1937  loss =  1.0650180578231812\n",
      "epoch =  59  step =  800  of total steps  1937  loss =  0.8406054973602295\n",
      "epoch =  59  step =  1000  of total steps  1937  loss =  1.097373366355896\n",
      "epoch =  59  step =  1200  of total steps  1937  loss =  0.9638849496841431\n",
      "epoch =  59  step =  1400  of total steps  1937  loss =  0.6276075839996338\n",
      "epoch =  59  step =  1600  of total steps  1937  loss =  0.7593082785606384\n",
      "epoch =  59  step =  1800  of total steps  1937  loss =  1.1405972242355347\n",
      "epoch :  59  /  100  | TL :  0.9012742872913904  | VL :  0.9141395092010498\n",
      "epoch =  60  step =  0  of total steps  1937  loss =  1.0764158964157104\n",
      "epoch =  60  step =  200  of total steps  1937  loss =  1.0663068294525146\n",
      "epoch =  60  step =  400  of total steps  1937  loss =  1.4183149337768555\n",
      "epoch =  60  step =  600  of total steps  1937  loss =  0.6575527191162109\n",
      "epoch =  60  step =  800  of total steps  1937  loss =  0.82218998670578\n",
      "epoch =  60  step =  1000  of total steps  1937  loss =  1.2666783332824707\n",
      "epoch =  60  step =  1200  of total steps  1937  loss =  0.5978717803955078\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-33e7d46154ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-55ee8c360b2e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreqd_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/envs/py36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1492\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/envs/py36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   2157\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m             \u001b[0;31m# if the dim was reduced, then pass a lower-dim the next time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/envs/py36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/envs/py36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_slice_axis\u001b[0;34m(self, slice_obj, axis)\u001b[0m\n\u001b[1;32m   2178\u001b[0m         \u001b[0mslice_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_slice_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2180\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iloc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/envs/py36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_slice\u001b[0;34m(self, obj, axis, kind)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/envs/py36/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_slice\u001b[0;34m(self, slobj, axis, kind)\u001b[0m\n\u001b[1;32m   3160\u001b[0m         \"\"\"\n\u001b[1;32m   3161\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3162\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3163\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/envs/py36/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mget_slice\u001b[0;34m(self, slobj, axis)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mslicer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mslicer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             \u001b[0mnew_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetitem_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/envs/py36/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mslicer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mslicer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             \u001b[0mnew_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetitem_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0mnew_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/envs/py36/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mgetitem_block\u001b[0;34m(self, slicer, new_mgr_locs)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Only same dim slicing is allowed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/envs/py36/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block_same_class\u001b[0;34m(self, values, placement, ndim, dtype)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mplacement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         return make_block(values, placement=placement, ndim=ndim,\n\u001b[0;32m--> 235\u001b[0;31m                           klass=self.__class__, dtype=dtype)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__unicode__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "total_step = len(trainset) // (train_batch_size * reqd_len)\n",
    "train_loss_list = list()\n",
    "val_loss_list = list()\n",
    "min_val = 100\n",
    "for epoch in range(num_epochs):\n",
    "    trn = []\n",
    "    Net.train()\n",
    "    for i, (images, labels) in enumerate(trainloader) :\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images).cuda().float()\n",
    "            labels = Variable(labels).cuda()\n",
    "        else : \n",
    "            images = Variable(images).float()\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        _, target = torch.max(labels, 1)\n",
    "\n",
    "        y_pred = Net(images)\n",
    "        \n",
    "        loss = criterion(y_pred, target)\n",
    "        trn.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(Net.parameters(), 10)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 200 == 0 :\n",
    "            print('epoch = ', epoch, ' step = ', i, ' of total steps ', total_step, ' loss = ', loss.item())\n",
    "            \n",
    "    train_loss = (sum(trn) / len(trn))\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    Net.eval()\n",
    "    val = []\n",
    "    with torch.no_grad() :\n",
    "        for i, (images, labels) in enumerate(valloader) :\n",
    "            if torch.cuda.is_available():\n",
    "                images = Variable(images).cuda().float()\n",
    "                labels = Variable(labels).cuda()\n",
    "            else : \n",
    "                images = Variable(images).float()\n",
    "                labels = Variable(labels)\n",
    "                \n",
    "            _, target = torch.max(labels, 1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = Net(images)\n",
    "            loss = criterion(outputs, target)\n",
    "            val.append(loss)\n",
    "\n",
    "    val_loss = (sum(val) / len(val)).item()\n",
    "    val_loss_list.append(val_loss)\n",
    "    print('epoch : ', epoch, ' / ', num_epochs, ' | TL : ', train_loss, ' | VL : ', val_loss)\n",
    "    \n",
    "    if val_loss < min_val :\n",
    "        print('saving model')\n",
    "        min_val = val_loss\n",
    "        torch.save(Net.state_dict(), '../saved_models/model0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faf80bdd048>,\n",
       " <matplotlib.lines.Line2D at 0x7faf80bdd198>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deVhV1f7H8fdCEGVWZBIEUnPIeUr0mtpsZXP5y25lNticzdNtsMFu5W3U22CllTctMyuzssEcKidwnhMHVEBFQQFBpvP9/bE5CDIqBw7n8H09z3mO7L3P3mspflisvdbaRkRQSinl+jycXQCllFKOoYGulFJuQgNdKaXchAa6Ukq5CQ10pZRyE57OunCrVq0kNjbWWZdXSimXtHLlyoMiElLRPqcFemxsLAkJCc66vFJKuSRjTFJl+7TLRSml3IQGulJKuQkNdKWUchMa6Eop5SY00JVSyk1ooCullJvQQFdKKTfheoG+fj08/TQcPOjskiilVIPieoG+bRuMHw979zq7JEop1aC4XqAHBVnvhw87txxKKdXAaKArpZSbcL1Ab9HCetdAV0qpMlwv0LWFrpRSFXK9QA8IsN4zMpxbDqWUamCqDXRjTBtjzAJjzGZjzEZjzNgKjjHGmHeMMYnGmHXGmN51U1ygSRMr1LWFrpRSZdRkPfRC4GERWWWM8QdWGmN+FZFNpY65CDi9+NUfeK/4vW4EBWmgK6XUCaptoYtIqoisKv5zFrAZiDzhsMuBz8SyDAgyxkQ4vLR2LVpooCul1AlOqg/dGBML9AKWn7ArEthT6uu9lA99x9EWulJKlVPjQDfG+AFfAw+ISOaJuyv4iFRwjjHGmARjTEJaWtrJlbS0oCC9KaqUUieoUaAbY7ywwvxzEZldwSF7gTalvo4CUk48SEQmi0hfEekbElLhM05rRlvoSilVTk1GuRjgY2CziLxRyWFzgJuKR7vEAUdEJNWB5SxL+9CVUqqcmoxy+QdwI7DeGLOmeNtTQDSAiLwP/AhcDCQCOcBoxxe1lKAgyMqCwkLwrEkVlFLK/VWbhiLyJxX3kZc+RoB7HFWoatlnix45AsHB9XZZpZRqyFxvpijo9H+llKqABrpSSrkJ1wx0XXFRKaXKcc1A1xa6UkqV49qBrpOLlFKqhGsHurbQlVKqhGsGur8/eHhooCulVCmuGejG6PR/pZQ6gWsGOmigK6XUCVw70PWmqFJKlXDtQNcWulJKlXDdQNcVF5VSqgzXDXRtoSulVBka6Eop5SZcO9BzciA/39klUUqpBsG1Ax20la6UUsVcN9B1xUWllCrDdQNdW+hKKVWGBrpSSrkJ1w90nS2qlFKAKwe69qErpVQZrhvo2uWilFJluG6gN28OXl4a6EopVcx1A13XRFdKqTJcN9BBl9BVSqlSXDvQdcVFpZQqUW2gG2OmGGMOGGM2VLI/0BjzvTFmrTFmozFmtOOLWQntclFKqRI1aaF/AgyrYv89wCYR6QEMBV43xjStfdFqQANdKaVKVBvoIrIYSK/qEMDfGGMAv+JjCx1TvGpooCulVAlH9KFPAjoDKcB6YKyI2Co60BgzxhiTYIxJSEtLq/2VW7SwboqK1P5cSinl4hwR6BcCa4DWQE9gkjEmoKIDRWSyiPQVkb4hISG1v3JQkLUe+rFjtT+XUkq5OEcE+mhgtlgSgZ1AJwect3o6W1QppUo4ItB3A+cCGGPCgI7ADgect3oa6EopVcKzugOMMTOwRq+0MsbsBZ4DvABE5H3gReATY8x6wACPi8jBOitxaRroSilVotpAF5GR1exPAS5wWIlqoMhWhDEGD/uKizpbVCmlXG+m6Fcbv8L7JW+2p2/XFrpSSpXicoHeonkLiqSI1OxUDXSllCrF5QK9tX9rAFKyUjTQlVKqFJcL9Ai/CKA40L29rXXRtQ9dKaVcL9CDmgXRzLMZqVmpxRt0+r9SSoELBroxhtb+rUnJTrE2aKArpRTggoEOVreLttCVUqoslwz01v6trT500EBXSqliLhnoEX4R1rBFOL7iolJKNXIuGeit/VuTmZdJdn62ttCVUqqYSwZ6hL81dDE1K/V4oOua6EqpRs4lA90+uahktmhRERw96uRSKaWUc7l0oOtsUaWUOs4lA90+WzQ1K9W6KQp6Y1Qp1ei5ZKDbZ4tqC10ppY5zyUA3xhwfuqiBrpRSgIsGOpSaXKSBrpRSgDsEur0PXQNdKdXIuWygl3S5BAZaG/SmqFKqkXPZQLfPFj1qywM/P22hK6UaPZcN9JLZotmpOv1fKaVw4UAvN7lIA10p1ci5bKCXm1ykga6UauRcNtDLtdD1pqhSqpFz2UAvN1tUW+hKqUau2kA3xkwxxhwwxmyo4pihxpg1xpiNxphFji1ipdcsO1tUA10p1cjVpIX+CTCssp3GmCDgXeAyEekCXOuYolWvzOSiI0fAZquvSyulVINTbaCLyGIgvYpDrgdmi8ju4uMPOKhs1YrwL9VCF4GsrPq6tFJKNTiO6EPvALQwxiw0xqw0xtxU2YHGmDHGmARjTEJaWlqtL9za74T1XPTGqFKqEXNEoHsCfYBLgAuBZ4wxHSo6UEQmi0hfEekbEhJS6wuXzBYNaGZt0H50pVQj5ohA3wvME5GjInIQWAz0cMB5q1UyW7R5kbVBA10p1Yg5ItC/A84yxngaY3yA/sBmB5y3WiVj0ZvmWRvSq+rqV0op9+ZZ3QHGmBnAUKCVMWYv8BzgBSAi74vIZmPMPGAdYAM+EpFKhzg6UslsUT+xNiQl1cdllVKqQao20EVkZA2OmQBMcEiJTkJJC10yISAAduyo7yIopVSD4bIzRcGaLerdxJvU7H3Qrh1s3+7sIimllNO4dKAbY45PLtJAV0o1ci4d6GB1u6Rmp0LbtrBrFxQVObtISinlFC4f6BH+Ecdb6Pn5kJzs7CIppZRTuHygl8wWbdfO2qDdLkqpRsrlAz3CP8KaLRptDWHUkS5KqcbK5QPdPnQxNdADPD21ha6UarTcJ9Bz0yA2VgNdKdVouXyg22eL6tBFpVRj5/KBXtJCtw9d1D50pVQj5fKBbp8tWtJCz8jQddGVUo2Sywd6udmioN0uSqlGyeUDHUo9iq5tW2uDdrsopRohtwj0kha6PdC1ha6UaoTcI9D9WpOalQp+fhAWpoGulGqU3CLQI/wjOJJ3hJyCHKuVroGulGqE3CLQS4YuZqVaN0a1D10p1Qi5RaDbJxclZyVbgb5nD+TlOblUSilVv9wi0Du26oinhyejvh3FjxHZIGKtja6UUo2IWwR6dGA0v9/0O809m3PJvte59lpI2RLv7GIppVS9cotABzgr5izW3LmGl/o9ztwO0Gnt7UxcPpEimz7BSCnVOLhNoAM0bdKUf130bzZMbcaA/FDun3c/7yx/x9nFUkqpeuFWgQ6AMbRr2Z5563vQLbQbPyb+6OwSKaVUvXC/QAdo1w6zfQdDY4eyZM8S8ovynV0ipZSqc24b6OzYwdCYIeQU5JCQkuDsEimlVJ2rNtCNMVOMMQeMMRuqOa6fMabIGHON44p3itq2hdxcBjfrCMDCXQudWx6llKoHNWmhfwIMq+oAY0wT4FXgZweUqfaKl9FtlXKYrqFdWZS0yMkFUkqpuldtoIvIYiC9msPuA74GDjiiULVWal30oTFD+XP3nxQUFTi3TEopVcdq3YdujIkErgTer8GxY4wxCcaYhLS0tNpeunIxMeDhYQV67FDtR1dKNQqOuCn6FvC4iFQ7g0dEJotIXxHpGxIS4oBLV6JpU2jTBnbsYHDMYADtdlFKuT1HBHpf4AtjzC7gGuBdY8wVDjhv7bRrB9u3E+IbQpeQLnpjVCnl9mod6CJymojEikgsMAu4W0S+rXXJaqs40AGGxmo/ulLK/dVk2OIMYCnQ0Riz1xhzqzHmTmPMnXVfvFpo2xbS0iAriyExQzhacJRVqaucXSqllKozntUdICIja3oyEbm5VqVxJPtIlx07GNJ+CGCNR+8f1d+JhVJKqbrjnjNFoczQxVDfUM4IOYOFSQudWiSllKpL7hvobdta71u2ADAkZgh/7v6TQluhEwullFJ1x30DPSgIevWC774DrBuj2fnZ2o+ulHJb7hvoANdfDytWQGIiQ2KO96MrpZQ7cu9Av+46MAZmzCDML4xOrTrpBCOllNty70CPioLBg+Hzz0GEoTFD+SPpD+1HV0q5JfcOdLC6XbZuhTVrGBo7lKz8LFanrnZ2qZRSyuHcP9Cvvhq8vGD6dIbEWv3ojbHbZf3+9WTkZji7GEqpOuT+gR4cDMOGwYwZhPuE0iG4A4uTFju7VPVKRDhr6ln8+89/O7soSqk65P6BDla3S3Iy/PEHA6IGsDx5OSLi7FLVm/TcdI7kHSExPdHZRVFK1aHGEeiXXgq+vjB9OnFRcRw4eoBdh3c5u1T1JjkrGYDdR3Y7uSRKqbrUOALd1xeuuAK++oq4sD4ALNu7zMmFqj/JmRroSjUGjSPQwep2ycig6+pkfLx8GlegF7fQ03LSyC3IdXJplFJ1pfEE+vnnQ3AwnjO+pF/rfixLbkSBXtxCB9iTuceJJVFK1aXGE+heXjBiBHz3HXGhvVmduppjhcecXap6YW+hg3a7KOXOGk+gg9XtkptL3B6hwFbQaCYYJWcl06JZC0ADXSl31rgCfeBAiI2l/8fzgMZzYzQ5M5l+kf0wGA10pdxY4wp0Dw+YNImI+C1E2wJYnrzc2SWqFylZKcQGxtLav7UGulJurHEFOsAll8DttxO3KZNl22u/BMCRY0e4bc5tHDh6wAGFc7y8wjzSctKIDIgkOjBaA10pN9b4Ah3g9deJy21J0rF9pKZuq9Wp5iXO4+PVHzN9/XQHFc6xUrNTAYj010BXyt01zkD39yfurvEALP/3PbU6VXxKPGAFe0NkH7JYuoXemJY9UKoxaZyBDvS68Ga8xINlm3+FuXNP+Tz2QF+UtKhBTtqxD1m0t9DziqwuGKWU+2m0gd7Msxm9WvdhWUdfuO02OHjwpM9RZCtiZcpKOgZ35FjhsQa5imPpFnqbgDaADl1Uyl012kAHiGszgPjwIgozDlmhbrOd1Oe3HNzC0YKjPBj3IM08mzXIbpfkrGSaeTajRbMWRAdGAxroSrmrxh3oUXHkFB1jw8sPwHffwbhxJ/X5FckrABgSO4QhMUOYt71hBnqkfyTGGA10pdxcow90gGWD28Lo0fDii/DFFzX+fHxKPAHeAXQI7sCw9sPYcnBLg1uWNzkzmciASABaNm+Jj5ePBrpSbqraQDfGTDHGHDDGbKhk/z+NMeuKX0uMMT0cX8y6ERsUS6hvKMuSl8N778GgQVawr1hRo8/Hp8TTJ6IPHsaDYe2HAfBz4s91WeSTZm+hAyWtdF2gSyn3VJMW+ifAsCr27wSGiEh34EVgsgPKVS+MMfSP7G8tAeDtDbNnQ3i4tXZ6cnKVn80rzGPtvrX0a90PgI7BHYkJjGlQ3S4iYrXQiwMd0LHoSrmxagNdRBYD6VXsXyIi9qcPLwOiHFS2ehEXFcfWQ1tJz02HkBD4/nvIyoLLL4ecnEo/t27/OgpsBfSLtALdGMOw9sOYv2M+BUUF9VX8KqXnppNXlFfS5QIQHaCBrpS7cnQf+q3AT5XtNMaMMcYkGGMS0tIaxlhoez+6/QYnXbvCjBmwahWMGlXpyBf7+HN7Cx1gWPthZOVnsXTv0rotdA2VHoNuFx0Yzb7sfeQV5jmrWEqpOuKwQDfGnI0V6I9XdoyITBaRviLSNyQkxFGXrpV+ra1VCJfvLbVQ1/DhMGECzJoFDz4IFcysjE+JJ8QnpGTkCMA5p52Dp4dngxm+WHoMup29vHsz91b4mYzcDG3BK+WiPB1xEmNMd+Aj4CIROeSIc9YXf29/uoZ25cNVH7LryC7aBLQhOjCaNpd1oVfKHYS+8Q6EhcFTT5X53IrkFdaStMaUbAvwDmBgm4HMS5zHy+e+XN9VKaeyFjpYQxfbtWxX7jP3/HgPy/YuY8fYHfVTSKWUw9Q60I0x0cBs4EYR+bv2Rap/jw58lHcT3uWX7b+QmpWKYLXIW4W3YttNIwj6178gNNSafARk5WWxOW0zI84YUe5cw9oN46nfn2Jf9j7C/cLrtR4nsrfQI/wjSrZVNRZdRPhtx2+k5aSxN3MvUQEudTtEqUavJsMWZwBLgY7GmL3GmFuNMXcaY+4sPuRZIBh41xizxhiTUIflrRM39riRpbcuJfmhZPKezmPX2F18de1XHMw5yORbe8KwYXDHHfDttwCsSl2FICU3REuzD1/8Zfsv9VqHiiRnJRPqG0rTJk1LttlDuqJA33JwS8k6L2W6oJRSLqHaFrqIjKxm/23AbQ4rkZN5NfEiJiiGmKAYzmt7Hm8nTOKBLzfQ9PxhcN118MsvxHuWvyFq1yO8B2G+YcxLnMdNPW6q7+KXkZyVTGv/1mW2eXt6E+4XXmGgL0qy1oc3GJYnL+fqM66ul3IqpRyjUc8Urc4jAx4hJSuF6Tu+gx9+gNhYGD6c+L9mEhMYQ4hv+Ru7HsaDC9tfyC/bf6HIVlT/hS7lxDHodtGB0ezOrDjQW/u35szIMxvN4/mUcica6FW4oN0FdA/rzn+W/AcJDoZff4UePYhPjqdfYg4kJlb4uWHthnEo9xCrUlfVc4nLKj1LtLQ2AW3KtdBFhEW7FjEkZghxUXEkpCRQaCusr6IqpRxAA70KxhgeGfAIG9M2WkMR27Th4E9fs7MF9NucCd26wSuvQEHZiUTntzsfTw9Ppq6Z6qSSWzNZD+YcLDNk0a6iB10kpieSmp3K4JjBxEXFkVuYy/r96+uzyEqpWtJAr8b/df0/Iv0jmbBkAgDxqSsB6PfqNLj4YnjySejbF/74o+QzrXxaMab3GD5c9SGJ6RW34uta6UfPnSg6MJqcghxrdmwx+1ruQ2KG0D+yP0CjeYi2Uu5CA70aTZs05YG4B1iwawErU1YSnxKPwdCn24Xw9dfwzTeQng6DB8O118LOnQA8M+QZmjZpyjMLnnFKuSuaVGRX0dDFRUmLCPUNpVOrTscXLdN+dKVcigZ6DYzpM4YA7wD+s/Q/xKfE06lVJwK8A6ydV1wBW7fCCy/Ajz9Cp07w+OOE23x4MO5BvtjwBatTV1d6bhGpk5unFU0qsqss0AfHDMYYU7JombbQlXItGug1EOAdwB197mDmxpksTlpcfvy5jw888wxs2wbXXw+vvQbt2/Po3yG0bN6SJ+c/WeF503PTOfOjM7nsi8scXuaTaaHvOryL3Ud2Mzh6cMkxcVFxbDm4hYzcjHKfV0o1TBroNXR///vxMB5k5mVWOP4cgNatYepUSEiATp0IvOsBnlrhzc/bf2bBzgVlDs3Ky+Kizy8iISWBH7f9yJI9S6q8fnZ+NvN3zK9xeUs/eu5EIT4heDfxLgn0Rbus8edDYoeUHGPvR7cvQqaUavg00GsoKiCK67tdD1Q8oaiMPn1g0SL48kvuWeVJ1BF44v2rkR3W+ig5BTlcOuNSVqasZPpV0wluHsyrf71a5Snv/uFuzpt2Hj9tq3QxyzJKP3ruRPYHXdjHoi9OWkzL5i3pGtq15Jh+kdaiZdqPrpTr0EA/CePPGc9zQ56jb+u+1R9sDIwYQbONW3k+6ApW+GTwzRUdyfu/q7n61d4sTlrMtIs+ZGS3kdx35n3M2TqHjQc2VniqVamrmLZuGgbDw788XKP11ks/eq4ipR90sShpEWdFn4WHOf7tEOAdwBkhZ2g/ulIuRAP9JEQFRDFu6DiaeDSp+YeaN+emp7+iU2B7nrrMl+t95jGvaCuT5wgj426D7t25d34mPp4+vLbktXIfFxEe/fVRgpsHM/XyqWw+uJnJK6t/KFRlk4rs7IGenJnM9oztDIkZUu6Y/pH9Wb53eZnx6kqphksDvR54enjy8oWvsdXrCLNjc3hz4Avc9sJcePppCAsj+KU3GLPaMH3t5yRl7Crz2Z8Sf+L3nb/z7JBnuanHTZwdezbPLXyuypuVFT167kTRgdGkZqXy247fABgcM7jcMXFRcRzKPcT2jO2nVnGlVL3SQK8nV3S6gtt7387bw97mgfOfgUsugeeft5YTWLKEh/a1haIi3nhkIKy2hjkW2gp59NdHad+yPXf2vRNjDG9c+Abpuem8tPilSq9V0aPnThQdGI0gTN8wnQDvAHqG9yx3TP8o68ao9qMr5Ro00OuJMYbJl07m/v73l985YABtFq/hBv+BfBiRysFBveHyy5n6+AVsStvEK0Vn0/Tb7yEhgZ6h3bml1y1MXDGRbYe2VXitqsag29mHLv624zcGRQ+qsBupS0gXfL18dSldpVyEBnpD4eHBYzd/RK4XTBwbR/bfG3jWLGTgbrjqwQ/hmmugXz9o146Xlvvi7eHFo78+WuGpqhqDbmcPdJvYKuw/B2ji0YR+kf1YlqwtdKVcgQZ6A9I5pDOXd7yciYFbef7tq9jnK0z410JMcjKsWQOffQYdOxL+yiSe/DmH77Z+x4IJd0Nqapnz1KSFXvppRBX1n9vFRcaxZt8acgtya1m7hu33nb83iIeSKFUbGugNzBODniDjWAb/WfofrjnjGga2HWJNWOrRA268EebNg+RkHrzyVaJzmvLgjvcoahMJF1xgBX5WVoWPnjuRj5cPrXxa4evlS5+IPpUe1z+qP4W2QlbvK7t8wdI9S3no54fIzMt0TMWdSES45btbGPXtKGxic3ZxlDplGugNTFxUHENihuDl4cW/z/13xQeFh9P8wceYcNM01obDLf/qSlHi3zBqFISFkfztZ4TgS9MvZ1mrQO7cCfn55U7TJaQL57Y9F68mXpWWp2TlxeJ+9PTcdMZ8P4aBUwby5rI3eeZ35yw+5kjxKfEkHUliX/a+amfsKtWQ1foh0crxPr3iU3Yd3kX7lu2rPG5ElxFsPbiVZxc+S/5r1/FZ+Gd4Tf+C5MyPiTyWD+P+efxgY6wHXUdFQWQkREXxTeuzaPKPQSBi7a9AhH8E0YHRLN27lOC1wTzyyyOk56bz8ICHycjNYFL8JEb1HEXviN6O/CuoVzM3zsTLwwsP48GsTbMYFD3I2UVS6pQYZ00a6du3ryQkuNzzpBukCX9N4LHfHuPKTlcy4+oZ9P+oP1E+4czt9ybs2XP8lZwMe/dar+RkyCgeyz5wIDzxhDWU0qP8L23/N+v/mLlxJgADogbw/vD36R7WncPHDtNxUkdiAmNYeuvSk5tw1UCICLFvx9IttBsexoM1+9aQ9EBShUsmKNUQGGNWikiF09W1he4GHv3HozTzbMb98+7nqplXsSdzj9VV0rmz9apMVhZMmwYTJsBll0GXLvD449bDsL2Od8Nc2elK/tz9J+OGjOPW3reWLBEQ1CyINy98k3/O/ieTV07mrn531XVVK2QT2/EVI2MGl1nCoDorklew+8huXjz7RQC+//t74lPiOTPyzLoqrlJ1RgPdTdzX/z68Pb25c+6dCFLlkMUS/v5w990wZgx8+aX1OL2bboI77oA2bazumagorouK4rr2L0H0cDghLEd2HcmU1VN4cv6TXNn5SsL9wuuohpaj+UdZs28Nq/etZt3+daw/sJ71+9dztOAoAJOHT+b2PrfX+Hz27pbLOl6GiODp4cnXm752mUBPOpzEswuf5a0L36JF8/Ira6rGRbtc3Mynaz7lljm3MOPqGYzoMuLkPixiPaTj99/Ld88UFlrdMWefbT2Z6aqrICQEgK0Ht9L9/e5ce8a1/O+q/1V7mYKiAhYlLaKVT6sKZ6iWlnQ4iTlb55CQmsDKlJVsPri5ZCRKy+Yt6R7WnW6h3ege1p13498lMy+TLfduwdOj+raKiBDzVgw9wnvw/cjvARj2v2Ekpiey7b5tLtHtYu8Oe2HoCzwzxPVvUKvqVdXlooHuhg7lHKJl85aOCySbDdatg1mz4Kuv4O+/rXDv3x+8vSEvj2djd/Jix338tqAN5wb0tIZZ9ix+b9uWIoTFSYv5YsMXzNo8q+R5poOiBzG2/1iu6HRFSQiLCAt3LeSdFe8wZ+scbGIjzDeMvq370ieiD31b96V3RG9a+7cuU8fZm2dz9cyrmXH1DK7rel211Vq2dxkDPh7Ap1d8yk09bgLgw5UfMmbuGFbfsbraHzbOFp8cz5kfnYmPlw8+Xj7sfmA3zb2aO7tYqo5poCvHEYH162HmTFi40Ap2b29ym3nSrdsfNMHw4spAMjJSONxUyGgOBwI9+amDYZ93Ab7Gm8vbnM+IuFvYcXgXE1dMZOfhnUQHRnNPv3sI9A5kUvwkNhzYQHDzYMb0GcNtvW/jtKDTqv0BZRMbXd7tgncTb1bfsbra4x/++WEmxU9i/yP7CWoWBEDa0TTCXw/nqUFP8eI5Lzrqb83hRIRzPzuXDQc2MPXyqQyfMZx3L37XafcxVP2pVaAbY6YAw4EDItK1gv0GeBu4GMgBbhaRVdUVSgPd/fyc+DPDPh9WZpsXTWhha8o/0ppz3bIsLtlQgG8B4OcHvXtT1KcXczt68LZtCQsOWGPde4X34v7+93Nd1+to5tnspMrwyZpPGP3daH68/kcuOv2iSo+ziY3Yt2LpGd6TOSPnlNl3zqfnsC97H5vu2XRS165P8xLncdHnFzHxoonc0+8e4j6O41DOIbbeu9Who41EhM0HNzP377kUFBXw1FlPuURXlDurbaAPBrKBzyoJ9IuB+7ACvT/wtoj0r65QGujuaXOa1cfdonkLgpoF0dyz+fEAKCyEzZutR/QlJMCqVdaSBseOAbAxpjk5Pc6gb9/LMeedB337lhltUxP5Rfm0e6cdpwWdxuLRiys9bumepQycMpDPrviMG3vcWGbff1f8l3t/updNd2+ic0gVo4ScxCY2en3Qi+z8bDbfs5mmTZry9aavueara/jq2q+45oxranX+/KJ8FuxcwNy/5/LDth/YeXhnyb6lty4lLiqutlWoVEZuBoW2QkJ8Q+rsGq6uqkBHRKp9AbHAhkr2fQCMLPX1ViCiunP26dNHlJKCApF160SmThW5916Rnj1FrI4dEX9/kUsuEXnlFZHvvxfZts06vhpvLX1LGIf8mfRnpcc8OO9BafpiUzfsLuQAABJzSURBVDmce7jcvuTMZGEc8uKiF2tTs1r5I+kP+SDhAykoKl/faWunCeOQGetnlGwrLCqU9u+0l36T+4nNZjvl6+YV5sngqYOFcUjzl5rLpdMvlffj35fNaZsl4N8B8s+v/3nK567O/uz9EvNmjLR+vbWk56TX2XVcHZAgleRqjfrQjTGxwFypuIU+F3hFRP4s/no+8LiIlGt+G2PGAGMAoqOj+yQlJVV7bdUIHTxo9c///jvMn2/dhLVr2hROPx06dLBmvoaEQKtW1is4GLy8OGo7RsyK64kLOIO5vSZA27YQEVEyG9YmNmLeiqFXeK9y3S12/5jyD2uI5J1rKtwvIqzdv5afE3/mlx2/sG7/Oj694lMuPv3iWlf/h79/4KqZV5FflE+/1v2YcvmUkue95hXm0XFSR4J9gom/Pb7MmPv34t/j7h/vZuGohWUe+F1TIsKY78fw0eqP+O/F/2V0z9FlbrKO/Wks7yW8x54H9xDmF1brepaWV5jHedPOIyElgUJbISO6jODzqz536DXcRV230H8ABpX6ej7Qp7pzagtd1Vh6usiSJSJTpog89pjIpZeKdO4sEhIi4uFxvEVf6vXCYIRxyNqw4m2hoSIXXijyxBOyZOqLwjhk2tpplV7y9SWvC+OQxEOJJdvyCvNkzpY5ctM3N0nYhDBhnHWNbu92kw4TO4jPeB9ZtmdZrar6zeZvxOsFL+nzQR+ZsmqKtHqtlXi94CUvLHxB8gvz5c2lbwrjkF+3/1ruszn5ORLyWohc8vklp3TticsnCuOQp357qsL9W9K21MlvLjabTUZ/O1oYh3yx/gt5fuHzwjhk5oaZDr2Ou6CKFrp2uSjXVlQkcvCgyJYtIn/9JbJ4scjChZL+yxzxe7G5XD9xqMhbb4mMGiXSvbvYPJvIfRch3k8jR4afLzJ9usjRo+VOuytjlzAOeeWPV2TJ7iVy99y7JfjVYGEc0vLVlnLdrOtk6uqpkpyZLCIi+7L2Sdu320rwq8GyJW1LhUXNPJYpLy16SSYtn1RhV8/MDTPF8wVPifsoTjJyM0RE5ED2ARk5a6QwDunxXg8JfjVYzv/s/Er/OuxhuGH/hpP6a5y/Y740eb6JXDr9UimyFVV63AXTLpDI1yMr7Ao6VfYfns/8/oyIiOQX5ku/yf0k+NVgSclMqfAzW9K2yNvL3nZoOVxFXQf6JcBPgAHigBU1OacGuqprj/z8iHg87yFP/vakjJw1Uvp80Ef8X/YXxiGXP3O6SFSU9V/Az88K/IkTRV5+WeTxx0Xuukv6PtFSzHNWK7zZi95y3azrZO7WuZJfmF/h9bYd2iahE0Il+s1o2Xtkb5l987bNk+g3o0ta9b7jfeX2ObfLqpRVIiLyv7X/E4/nPWTQlEFy5NiRcuf+dvO3Ev6fcGEcsjJlZaV1Pnj0oPiM95Gbv725xn9PiYcSpeWrLaXLf7tUeO3S5myZI4xDZm2cVePzV+WHv38Qj+c95Oovry7zg2Rz2mZp9lIzueTzS8rdE/hk9SfiM95HGIeMXzz+lK5bm/sMzlZVoNdklMsMYCjQCtgPPAd4FXfXvF88bHESMAxr2OJoqaD//EQ6ykXVtZSsFDpO6sjR/KPEBsXSIbgDHYI7cHrL07mq81VE+kXA4sXwv/9ZE6Yyi9d29/KCwEBmd23C1Jh0rl5bwFWbISA8BoYMsRYza9MGwsOtV2goeFqTolalrmLIJ0OIDYrlj9F/YBMbD/38EJ+u/ZTOQafzcfAteGF4L+cPZhyYT27RMXqG92TtvrUMjR3KnJFz8GvqV2F9Dh87zI6MHdWubHnfj/fxwcoPeGLQExTZiii0FZa8wvzCOL3l6XQI7kD7lu2xiY0BHw8gNTuVFbetoF3LdlWeu8hWRPuJ7YkNimXBqAUn/49Syqa0TcR9FEf7lu35Y/Qf+Db1LbP/neXvMHbeWD689ENu630bWXlZ3PPjPUxbN42hsUMJ9A7kh20/sPy25Se12ufq1NWMmDWCEWeMYPy540+63DaxsTdzb8lTv+qbTixSjdaRY0do5tkMb0/vqg/My4MjRyAw0Jr9amezwaZN1k3aRYus94MHy37WvjTxaadBu3b81hYubvIF3f3asfdoKods2TyxNoCnvz+Cd9Hxj2U0g2k9YHJfw2mFfnzpMwqfcy6EwYMhIOCU67zr8C56vN+DzLxMmpgmeDXxwtPDEw/jUe6BJP5N/ckpyOHnG37m3Lbn1uj8r/31Go//9jgb7tpAl9Aup1TGRbsWcf3s6ymyFRF/ezxtAtuUO8YmNs6fdj4rklfw2RWf8fhvj7M9YzvPDXmOf531L47kHaH7e90J8A5g5ZiVNZolO3vzbG785kbyCvMQhNV3rKZ7WPeTKvuTvz3Jq3+9ytcjvubKzlee1GcdQQNdKUcRgd27rcf+7dt3/D05GXbsgO3bYc8evugiXH819EqFj5e0omenoVbLfuBAaNYMDhywXvv3W6+VK+Gvv6wx+U2awJlnwtChEBdnLbEQdnKjSmxiw2DKTQLKzs8mMT2Rvw/9zbZD29iesZ3hHYZzVeeranzuQzmHiHozitE9R/PuJe+eVLkKbYW8sOgFxv8xnnYt2vHVtV/RI7xHpcfvPrKbbu91IzMvk9b+rZl+1fQyI3h+3f4rF/zvAsb2H8tbw96q9Dwiwst/vMzTC56mf2R/plw+hcFTB9M5pDOLb15c48lSqVmptHunHQW2Ajw9PPn9pt8Z0GZAzf8CHEADXan6lJcHSUkk/R1P5Bn98TytXaUPECnj2DFYutQaqjl/vjX5qrDQ2hcba4V7t25Wl5Ax1qt46QW6d4fevcHHp06rZnfLd7cwc+NMkh9KJrBZYI0+k3Q4ietnX8+SPUu4uefNTLxoYqXdS6XN/Xsuc7bO4eVzX6aVT6ty+8f+NJZ3VrzDrzf+ynltzyu3/1jhMW6bcxufr/+ckV1H8vFlH9PcqzkfrfqI27+/nWlXTuOG7jfUqA73/Xgf7698nz9H/8kN39xARm4GS25dQofgDjX6vCNooCvlinJzrdm0y5Ydf+3dW/nxTZpYa9r36wd9+oCvr/Ubhc1mvYP1tKquXa3n1NZiCv+q1FX0mdyHt4e9zf3976/wmLzCPA4cPcC+7H2s2beGR399FJvYeH/4+1zf7fpTvvaJcgty6T25N1l5Way7ax0tm7cErHso32/9ng9WfsDqfat56eyXyixdYL9/sPvIbrbeu5UA76q7uZIOJ3H6xNMZ3XM0H1z6AYnpiQz4eAAB3gEsvXUpob6hDqtTVTTQlXIXublWQNtD2maD7GxYvRri42HFCus9Pb3q8wQGWuHfpYvVX5+VZZ0nO9v6c2GhNYmraVPrN4CmTa3JWxdcAOeeC35+DPx4IPuy93Fzz5s5cPQAB44eYP/R/dZ79n4yjmWUuWT/yP5Mv3o6bVu0dfhfy8qUlcR9HMfwDsPpG9GXOX/PYUXyCgDatmjLhPMnVNitlJCSwJkfnskDcQ/wxoVvVHmN2+fczmfrPiPxvsSSPv/le5dz9qdn0zW0KwtGLSh3Y7cuaKAr1ZiIWC35/HyrS8beNSMCu3bBhg2wcaP12rDB6urx87MeeGJ/9/S0Pm9/5eVZ9wmys61wHzqUr8+L5JqcqYC1Nn2ob2jJK8w3jHC/cMJ8wwjzCyPCL4JeEb1qtE79qRq/eDxPL3gagDMjz+TyjpdzWcfL6BLSpco+8ju+v4OPV3/MmjvXlMzIPVFieiKdJnXinn738PZFb5fZN2frHK788krOb3s+155xLR7GA2MMHsYD7ybeXHz6xfh7+zusnhroSqnay8+HP/+EH36wHoSyZQvpzcHPy5emnbta/fvdusEZZ5TvyzcG2rWzRgPVkSJbEb9s/4We4T2J8I+o8ecO5Ryiw6QOdAvtxoJRCyoM/xtm38DszbPZMXZHhU/lei/+Pe796d6Sh6+U1iagDe8Pf98hy0KABrpSqi7s2GGtt7N+vfUAlPXr4dChqj/Tpo21imbfvlY/f0iI1b1TUGC9FxZCUVH5z3l7WyN9wsOt7iIHL+H7QcIH3PnDnXww/ANu7317mVDfeGAj3d7rxmP/eIxXznul0nMcyjlETkEONrGVvHYd3sX98+5nU9omRnYdydvD3q71SpIa6EqpuidiDeHcssUK6NIKC63tCQnWEM3SC66dLG/v45O6Wre2XpGRx987dLB+cJxE6BfZihg0dRDL9i6jS0gX7up7Fzf2uJEA7wCumXkNv2z/hZ1jdxLsE3zSxc0rzOOVP19h/B/jCfAO4M0L3+SG7jec8rryGuhKqYblyBHrRm5mptVf7+VlvXt6Hu/3Ly031xqvv2/f8VdqqvVKTobDh8se7+cHnTtbr44drVZ/Rsbx1+HD1nyA0FCr5R8aSm5IC77w3cG76T+TsG8Vvl6+XN7pcqavn85zQ55j3NBxtaryprRN3DbnNpbuXcpDcQ/x+oWvn9J5NNCVUu4tJ8cK9z17rN8ENm2yHqayaROkpFjH+PpCixbWKyjIuhlsn9iVl3f8XB4exA86jffO9GCG3y58PZuzPfZNAo8WWj+AMjOt3ziioiAmBqKjrfeAAOsH1e7dVjnsE9BCQ60lnNu1o6hNFO+tn8Kg6EGn/MxaDXSlVONlH5nTtGnF+0WsoZr791s/DOLjra6h+Hgysg9yzBMisksdb4w15t8+6cuu+IHpVTLG6g66/354+OFTqk5VgV53Y4iUUqoh8KtmNqoxVus6IMB6eMqll1rbRWixe7fVvWPfHxBgtfTB+gGQlGS1xJOSrKUcwsKsFnubNscXcEtLs24g25eG2LHDeuBKHdAWulJKuZCqWugeFW1USinlejTQlVLKTWigK6WUm9BAV0opN6GBrpRSbkIDXSml3IQGulJKuQkNdKWUchNOm1hkjEkDkk7x462Ag9Ue5Tq0Pg2XO9UF3Ks+7lQXqHl9YkSkwjV4nRbotWGMSahsppQr0vo0XO5UF3Cv+rhTXcAx9dEuF6WUchMa6Eop5SZcNdAnO7sADqb1abjcqS7gXvVxp7qAA+rjkn3oSimlynPVFrpSSqkTaKArpZSbcLlAN8YMM8ZsNcYkGmOecHZ5TpYxZoox5oAxZkOpbS2NMb8aY7YVv7dwZhlryhjTxhizwBiz2Riz0Rgztni7q9anmTFmhTFmbXF9ni/efpoxZnlxfb40xlTyLLOGxxjTxBiz2hgzt/hrV67LLmPMemPMGmNMQvE2V/1eCzLGzDLGbCn+/zPAEXVxqUA3xjQB/gtcBJwBjDTGnOHcUp20T4BhJ2x7ApgvIqcD84u/dgWFwMMi0hmIA+4p/vdw1frkAeeISA+gJzDMGBMHvAq8WVyfDOBWJ5bxZI0FNpf62pXrAnC2iPQsNV7bVb/X3gbmiUgnoAfWv1Ht6yIiLvMCBgA/l/r6SeBJZ5frFOoRC2wo9fVWIKL4zxHAVmeX8RTr9R1wvjvUB/ABVgH9sWbveRZvL/M92JBfQFRxMJwDzAWMq9aluLy7gFYnbHO57zUgANhJ8aAUR9bFpVroQCSwp9TXe4u3ubowEUkFKH4PdXJ5TpoxJhboBSzHhetT3EWxBjgA/ApsBw6LiP0R7670PfcW8BhgK/46GNetC4AAvxhjVhpjxhRvc8XvtbZAGjC1uDvsI2OMLw6oi6sFuqlgm467dDJjjB/wNfCAiGQ6uzy1ISJFItITq3V7JtC5osPqt1QnzxgzHDggIitLb67g0AZfl1L+ISK9sbpc7zHGDHZ2gU6RJ9AbeE9EegFHcVBXkasF+l6gTamvo4AUJ5XFkfYbYyIAit8POLk8NWaM8cIK889FZHbxZpetj52IHAYWYt0bCDLGeBbvcpXvuX8AlxljdgFfYHW7vIVr1gUAEUkpfj8AfIP1A9cVv9f2AntFZHnx17OwAr7WdXG1QI8HTi++U98UuA6Y4+QyOcIcYFTxn0dh9UU3eMYYA3wMbBaRN0rtctX6hBhjgor/3Bw4D+tm1QLgmuLDXKI+IvKkiESJSCzW/5PfReSfuGBdAIwxvsYYf/ufgQuADbjg95qI7AP2GGM6Fm86F9iEI+ri7BsEp3BD4WLgb6y+zX85uzynUP4ZQCpQgPWT+lasvs35wLbi95bOLmcN6zII61f2dcCa4tfFLlyf7sDq4vpsAJ4t3t4WWAEkAl8B3s4u60nWaygw15XrUlzutcWvjfb/+y78vdYTSCj+XvsWaOGIuujUf6WUchOu1uWilFKqEhroSinlJjTQlVLKTWigK6WUm9BAV0opN6GBrpRSbkIDXSml3MT/A1uZ0+SjTidfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = np.arange(60)\n",
    "plt.plot(j, train_loss_list, 'r', j, val_loss_list, 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv1d(3, 5, kernel_size=(3,), stride=(1,))\n",
       "  (conv2): Conv1d(5, 10, kernel_size=(3,), stride=(1,))\n",
       "  (fc1): Linear(in_features=460, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (pamap): Linear(in_features=64, out_features=12, bias=True)\n",
       "  (robogame): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_accuracy(dataloader, Net):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    Net.eval()\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = Variable(images).float().cuda()\n",
    "        labels = Variable(labels).float()\n",
    "\n",
    "        outputs = Net(images)\n",
    "        \n",
    "        outputs = outputs.cpu()\n",
    "    \n",
    "        _, label_ind = torch.max(labels, 1)\n",
    "        _, pred_ind = torch.max(outputs, 1)\n",
    "        \n",
    "        # converting to numpy arrays\n",
    "        label_ind = label_ind.data.numpy()\n",
    "        pred_ind = pred_ind.data.numpy()\n",
    "        \n",
    "        # get difference\n",
    "        diff_ind = label_ind - pred_ind\n",
    "        # correctly classified will be 1 and will get added\n",
    "        # incorrectly classified will be 0\n",
    "        correct += np.count_nonzero(diff_ind == 0)\n",
    "        total += len(diff_ind)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    # print(len(diff_ind))\n",
    "    return accuracy\n",
    "\n",
    "Net.cuda()\n",
    "# _get_accuracy(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7127645844088797\n",
      "0.7110020661157025\n",
      "0.706353305785124\n"
     ]
    }
   ],
   "source": [
    "print(_get_accuracy(trainloader, Net))\n",
    "print(_get_accuracy(testloader, Net))\n",
    "print(_get_accuracy(valloader, Net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.27995867768594 / 78.25413223140497 / 78.150826446281\n"
     ]
    }
   ],
   "source": [
    "testing_Net = ConvNet()\n",
    "testing_Net.load_state_dict(torch.load('../saved_models/model5.pt'))\n",
    "testing_Net.eval().cuda()\n",
    "print(_get_accuracy(trainloader, testing_Net) * 100, '/', _get_accuracy(valloader, testing_Net) * 100, '/', _get_accuracy(testloader, testing_Net) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
